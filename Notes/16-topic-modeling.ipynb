{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "anchor-section",
            "metadata": {},
            "source": [
                "# ðŸ“Œ Topic: Topic Modeling with LDA\n",
                "\n",
                "### What you will learn\n",
                "- What Latent Dirichlet Allocation (LDA) is and how it differs from LSA\n",
                "- The probabilistic approach to grouping words into themes\n",
                "- Building a complete pipeline: cleaning -> tokenization -> stemming -> LDA\n",
                "- How to evaluate topic coherence\n",
                "\n",
                "### Why this matters\n",
                "Organizations often have thousands of documents (news articles, customer emails, legal records) and don't know what's in them. **Topic Modeling** is a tool for unsupervised discovery. It helps you find the \"threads\" or themes that connect documents without needing a human to label them first. It's like having an automated librarian who can tell you, \"These 500 documents are about politics, and these 300 are about sports.\"\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "lda-intro",
            "metadata": {},
            "source": [
                "## What is LDA?\n",
                "\n",
                "**Latent Dirichlet Allocation (LDA)** assumes that every document is a mixture of several topics, and every topic is a mixture of several words. \n",
                "\n",
                "### The probabilistic view:\n",
                "1.  **Topics** are probability distributions over words (e.g., in a \"Sports\" topic, words like \"ball\" and \"goal\" have high probability).\n",
                "2.  **Documents** are probability distributions over topics (e.g., a news article might be 80% \"Politics\" and 20% \"Economics\")."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "cf84e121-0065-4155-81e3-e3fa29916ae4",
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "import re\n",
                "from nltk.corpus import stopwords\n",
                "from nltk.tokenize import word_tokenize\n",
                "from nltk.stem import PorterStemmer\n",
                "import gensim\n",
                "from gensim import corpora\n",
                "from gensim.models import LdaModel"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "data-loading",
            "metadata": {},
            "source": [
                "## Step 1: Preprocessing for Topic Modeling\n",
                "\n",
                "Topic modeling requires very clean data. If we keep \"the\", \"is\", and \"and\", the model will just find \"The grammar topic.\""
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "preprocessing-full",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load sample news articles\n",
                "data = pd.read_csv(\"news_articles.csv\")\n",
                "\n",
                "# 1. Clean characters\n",
                "articles = data['content'].str.lower().apply(lambda x: re.sub(r\"[^\\w\\s]\", \"\", x))\n",
                "\n",
                "# 2. Remove Stopwords\n",
                "en_stopwords = set(stopwords.words(\"english\"))\n",
                "articles = articles.apply(lambda x: ' '.join([word for word in x.split() if word not in en_stopwords]))\n",
                "\n",
                "# 3. Tokenize\n",
                "tokenized = articles.apply(word_tokenize)\n",
                "\n",
                "# 4. Stemming (reducing words to their roots)\n",
                "ps = PorterStemmer()\n",
                "processed_docs = tokenized.apply(lambda x: [ps.stem(word) for word in x])"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "dictionary-creation",
            "metadata": {},
            "source": [
                "## Step 2: Create Dictionary and Corpus\n",
                "\n",
                "Gensim needs a `Dictionary` (to map IDs to words) and a `Corpus` (word counts per document) to run LDA."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "efde7586-894f-4dad-a516-e5318a45b108",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Build the vocabulary dictionary\n",
                "dictionary = corpora.Dictionary(processed_docs)\n",
                "\n",
                "# Convert documents into Bag-of-Words vectors\n",
                "doc_term_matrix = [dictionary.doc2bow(text) for text in processed_docs]"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "training-lda",
            "metadata": {},
            "source": [
                "## Step 3: Run the LDA Model\n",
                "\n",
                "We'll try to find 5 distinct topics in our news corpus."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "lda-run",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Initialize and train the model\n",
                "lda_model = LdaModel(doc_term_matrix, num_topics=5, id2word=dictionary, passes=10)\n",
                "\n",
                "# Printing the top words for each topic\n",
                "for idx, topic in lda_model.print_topics(-1):\n",
                "    print(f\"Topic {idx}: {topic}\\n\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "key-takeaways",
            "metadata": {},
            "source": [
                "## Key Takeaways\n",
                "\n",
                "1.  **Probabilistic Soft Clustering**: Unlike LSA, documets can belong to multiple topics (e.g., 60% Topic A, 40% Topic B).\n",
                "2.  **Hyperparameters**: Choosing the number of topics (`num_topics`) is the most important part. Too few leads to broad topics; too many leads to redundant ones.\n",
                "3.  **Iterative Process**: You often need multiple \"passes\" over the data for the model to stabilize.\n",
                "\n",
                "## Next steps:\n",
                "- Try visualising your topics using `pyLDAvis`.\n",
                "- Experiment with **Lemmatization** instead of Stemming to see if the resulting topics are more readable."
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "nlp_course_env",
            "language": "python",
            "name": "nlp_course_env"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.11.14"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}