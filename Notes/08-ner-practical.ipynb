{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "anchor-section",
   "metadata": {},
   "source": [
   "# ðŸ“Œ Topic: Named Entity Recognition (NER) - Practical Applications\n",
   "\n",
   "### What you will learn\n",
   "1. **Real-world NER workflows** - Processing large datasets of news articles\n",
   "2. **Entity database construction** - Converting unstructured text into structured data\n",
   "3. **Entity analysis patterns** - Finding the most important entities in text\n",
   "4. **Production challenges** - Handling capitalization, abbreviations, and entity variations\n",
   "5. **Search implementation** - Building entity-based search across documents\n",
   "6. **Visualization strategies** - Making entity patterns visible and actionable\n",
   "\n",
   "### Why this matters\n",
   "NER transforms unstructured text into searchable, analyzable data. News aggregation platforms use NER to automatically index articles by people, companies, and locations. Search engines rely on entity extraction to improve relevance. Knowledge graphs depend on entity identification and linking. Understanding how to apply NER to real data bridges the gap between theory and production systems.\n",
   "\n",
   "### Real-world context\n",
   "When you search for \"Apple\" on a news website, the system uses NER to distinguish between Apple Inc. and the fruit. When a news aggregator groups articles by location, it extracted GPE (Geopolitical Entity) tags with NER. This notebook shows exactly how these systems work.\n",
   "\n",
   "### Data source\n",
   "BBC News dataset (1,000+ articles) with titles, publication dates, and descriptions. This represents real news text with all its messy characteristics: abbreviations, multi-word entities, inconsistent capitalization, and domain-specific terms.\n",
   "\n",
   "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": [
    "## Real-World Scenario\n",
    "\n",
    "Imagine you're building a **news analytics platform**:\n",
    "- Users want to search news by people mentioned (\"Find articles about Elon Musk\")\n",
    "- Editors need to organize articles by location (\"UK news\", \"Tech news from Silicon Valley\")\n",
    "- Algorithms need to find relationships (\"Which companies did this person work for?\")\n",
    "\n",
    "NER makes this possible by automatically extracting structured data from unstructured text.\n",
    "\n",
    "### What we'll do:\n",
    "1. Load BBC News dataset (1,000+ articles)\n",
    "2. Extract all entities using spaCy NER\n",
    "3. Build an entity database\n",
    "4. Analyze what entities appear most frequently\n",
    "5. Visualize entity distributions\n",
    "6. Handle edge cases and errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2fd5d99-8e1f-4329-a7e6-c8c64b390990",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries for data processing and NER\n",
    "import nltk  # NLTK for tokenization and text processing\n",
    "from nltk.tokenize import word_tokenize  # Split text into words\n",
    "from nltk.corpus import stopwords  # Common words to filter\n",
    "from nltk.stem import WordNetLemmatizer  # Normalize words\n",
    "\n",
    "import spacy  # Modern NLP library with NER\n",
    "import re  # Regular expressions for text cleaning\n",
    "import pandas as pd  # Data manipulation and analysis\n",
    "import matplotlib.pyplot as plt  # Visualization\n",
    "from collections import Counter  # Count entity frequencies\n",
    "\n",
    "print(\"âœ“ All libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "data-loading",\n",
   "metadata": {},\n",
   "source": [
    "## Step 1: Load Real News Data\n",
    "\n",
    "Before we extract entities, we need data. The BBC News dataset contains real articles with text that reflects production challenges:\n",
    "- Multiple mentions of the same entity with different capitalization\n",
    "- Abbreviations ('U.K.', 'U.S.', 'CEO')\n",
    "- Multi-word entities ('Boris Johnson', 'Google Assistant')\n",
    "- Domain-specific terms and variations\n",
    "\n",
    "Working with real data teaches you what production NER systems must handle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,\n",
   "id": "1cca17ae-8d35-403b-9d96-85f435db7380",\n",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load BBC News dataset\n",
    "# Data source: Data/bbc_news.csv\n",
    "# Contains: title, publication date, link, description for 1,000+ articles\n",
    "bbc_news = pd.read_csv(\"../Data/bbc_news.csv\")\n",
    "\n",
    "print(f\"âœ“ Loaded {len(bbc_news)} news articles\")\n",
    "print(f\"Columns: {list(bbc_news.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,\n",
   "id": "explore-data",\n",
   "metadata": {},\n",
   "outputs": [],
   "source": [
    "# Examine the data structure\n",
    "print(\"First 5 articles:\")\n",
    "print(bbc_news[['title', 'pubDate', 'description']].head())\n",
    "print(f\"\\nDataset shape: {bbc_news.shape}\")\n",
    "print(f\"\\nMissing values:\\n{bbc_news.isnull().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "load-nlp-model",\n",
   "metadata": {},\n",
   "source": [
    "## Step 2: Load spaCy NER Model\n",
    "\n",
    "### What is the spaCy NER model?\n",
    "spaCy's `en_core_web_sm` is a neural network trained on news text. It recognizes entity types:\n",
    "- **PERSON**: Individual names\n",
    "- **ORG**: Companies, institutions, governments\n",
    "- **GPE**: Countries, cities, states\n",
    "- **DATE**: Temporal references\n",
    "- **MONEY**: Monetary values\n",
    "- **PERCENT**: Percentages\n",
    "- **FACILITY**: Buildings, infrastructure\n",
    "- **PRODUCT**: Commercial products\n",
    "- **EVENT**: Named events\n",
    "- **LAW**: Laws and policies\n",
    "- **LANGUAGE**: Language names\n",
    "- **WORK_OF_ART**: Creative works\n",
    "- **NORP**: Nationalities, religious, political groups\n",
    "\n",
    "The model performs well on news text. On unfamiliar domains (medical, legal), it may struggle with specialized entities it never learned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,\n",
   "id": "load-spacy",\n",
   "metadata": {},\n",
   "outputs": [],\n",
   "source": [
    "# Load spaCy's NER model\n",
    "# This model was trained on news text, so it performs well on news articles\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "print(\"âœ“ spaCy model loaded\")\n",
    "print(f\"Pipeline components: {nlp.pipe_names}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "extract-entities",\n",
   "metadata": {},\n",
   "source": [
    "## Step 3: Extract Entities from All Articles\n",
    "\n",
    "### The NER Pipeline\n",
    "For each article, spaCy performs:\n",
    "1. **Tokenization**: Splits text into words and punctuation\n",
    "2. **Named Entity Recognition**: Identifies entity boundaries and types\n",
    "3. **Token Classification**: Labels each token as part of an entity or not\n",
    "\n",
    "This pipeline runs automatically; we just call `nlp(text)` and extract results.\n",
    "\n",
    "### What to expect\n",
    "- Processing 1,000+ articles takes 1-2 minutes (neural networks are compute-intensive)\n",
    "- Some entities will be misidentified (spaCy isn't perfectâ€”no NER model is)\n",
    "- Some obvious entities will be missed (depends on training data)\n",
    "- Real entities may have multiple variations in the text\n",
    "\n",
    "This is normal. Production systems add post-processing to handle errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,\n",
   "id": "entity-extraction",\n",
   "metadata": {},\n",
   "outputs": [],\n",
    "source": [
    "# Extract entities from all articles\n",
    "# This may take a minute or two for 1,000+ articles\n",
    "all_entities = []\n",
    "\n",
    "print(\"Extracting entities from articles...\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Process each article\n",
    "for idx, row in bbc_news.iterrows():\n",
    "    # Get article title and description\n",
    "    article_text = f\"{row['title']}. {row['description']}\"\n",
    "    \n",
    "    # Skip if text is missing\n",
    "    if pd.isna(article_text):\n",
    "        continue\n",
    "    \n",
    "    # Process text with spaCy NER\n",
    "    doc = nlp(article_text)\n",
    "    \n",
    "    # Extract entities\n",
    "    for entity in doc.ents:\n",
    "        all_entities.append({\n",
    "            'entity_text': entity.text,\n",
    "            'entity_type': entity.label_,\n",
    "            'article_title': row['title'],\n",
    "            'article_date': row['pubDate']\n",
    "        })\n",
    "    \n",
    "    # Progress indicator\n",
    "    if (idx + 1) % 100 == 0:\n",
    "        print(f\"Processed {idx + 1} articles, found {len(all_entities)} entities\")\n",
    "\n",
    "print(f\"\\nâœ“ Complete! Found {len(all_entities)} total entities\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "entity-database",\n",
   "metadata": {},\n",
   "source": [
    "## Step 4: Create Structured Entity Database\n",
    "\n",
    "### From text to data\n",
    "We've converted unstructured news articles into structured data:\n",
    "- **Before**: Raw text ('Barack Obama visited the UK')\n",
    "- **After**: Structured records with entity text, type, article source, and date\n",
    "\n",
    "Now we can query, search, filter, and analyze. This is why NER matters in production: it makes unstructured text searchable and analyzable.\n",
    "\n",
    "### Key metrics\n",
    "- **Total entities**: Count of all mentions (an entity mentioned 5 times = 5 entries)\n",
    "- **Unique entities**: Count of distinct entity texts (same entity across different articles)\n",
    "- **Distribution by type**: How many of each entity type did we find?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,\n",
   "id": "create-entity-db",\n",
   "metadata": {},\n",
   "outputs": [],\n",
    "source": [
    "# Convert to DataFrame for easier analysis\n",
    "entities_df = pd.DataFrame(all_entities)\n",
    "\n",
    "print(f\"Entity Database:\")\n",
    "print(f\"Total entities: {len(entities_df)}\")\n",
    "print(f\"Unique entities: {entities_df['entity_text'].nunique()}\")\n",
    "print(f\"\\nEntity type distribution:\")\n",
    "print(entities_df['entity_type'].value_counts())\n",
    "\n",
    "print(\"\\nSample entities:\")\n",
    "print(entities_df.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "entity-analysis",\n",
   "metadata": {},
   "source": [
    "## Step 5: Analyze Most Common Entities\n",
    "\n",
    "### Why frequency analysis matters\n",
    "Counting entity mentions reveals what news is about. If 'UK' appears 500 times and 'Iceland' appears 3 times, UK news dominates this dataset.\n",
    "\n",
    "**Use cases:**\n",
    "- **Editorial analysis**: What topics get coverage?\n",
    "- **Trend detection**: Which entities are increasingly mentioned?\n",
    "- **Search optimization**: Which entities should have dedicated index pages?\n",
    "- **Recommendation**: Similar articles linked by shared entities\n",
    "\n",
    "### Interpreting results\n",
    "High frequency doesn't always mean high importance. Frequent mentions might indicate:\n",
    "- Genuinely important news (major tech companies, major countries)\n",
    "- Recurring background context (the UK in BBC News dataset)\n",
    "- Common words that spaCy misidentifies as entities\n",
    "\n",
    "Always investigate the top results to understand what you're looking at."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,\n",
   "id": "top-entities",\n",
   "metadata": {},\n",
   "outputs": [],\n",
    "source": [
    "# Find most common entities overall\n",
    "entity_counts = entities_df['entity_text'].value_counts()\n",
    "\n",
    "print(\"Top 20 Most Mentioned Entities:\")\n",
    "print(\"=\" * 60)\n",
    "for entity, count in entity_counts.head(20).items():\n",
    "    print(f\"{entity:30} â†’ {count:4} mentions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "person-analysis",\n",
   "metadata": {},\n",
   "source": [
    "## Step 6: Analyze by Entity Type\n",
    "\n",
    "### Why entity types matter\n",
    "A company name has different implications than a location. By separating entities by type, you can answer:\n",
    "- \"Which people are mentioned most in tech news?\"\n",
    "- \"What organizations control the narrative?\"\n",
    "- \"What countries get the most coverage?\"\n",
    "- \"Which products are newsworthy?\"\n",
    "\n",
    "### Different use cases per type\n",
    "- **PERSON**: Relationship analysis, influence tracking, biography\n",
    "- **ORG**: Competitive analysis, industry structure, market power\n",
    "- **GPE**: Geographic analysis, regional focus, international news\n",
    "- **DATE**: Temporal patterns, event clustering\n",
    "- **PRODUCT**: Market analysis, technology trends\n",
    "- **MONEY**: Financial analysis, funding amounts, deal sizes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,\n",
   "id": "by-type-analysis",\n",
   "metadata": {},\n",
   "outputs": [],\n",
    "source": [
    "# Top PERSON entities (people mentioned)\n",
    "person_entities = entities_df[entities_df['entity_type'] == 'PERSON']['entity_text'].value_counts()\n",
    "print(\"Top 15 People Mentioned:\")\n",
    "print(\"=\" * 40)\n",
    "for person, count in person_entities.head(15).items():\n",
    "    print(f\"{person:25} â†’ {count:3} articles\")\n",
    "\n",
    "# Top ORG entities (organizations)\n",
    "print(\"\\n\\nTop 15 Organizations Mentioned:\")\n",
    "print(\"=\" * 40)\n",
    "org_entities = entities_df[entities_df['entity_type'] == 'ORG']['entity_text'].value_counts()\n",
    "for org, count in org_entities.head(15).items():\n",
    "    print(f\"{org:25} â†’ {count:3} articles\")\n",
    "\n",
    "# Top GPE entities (locations)\n",
    "print(\"\\n\\nTop 15 Locations Mentioned:\")\n",
    "print(\"=\" * 40)\n",
    "gpe_entities = entities_df[entities_df['entity_type'] == 'GPE']['entity_text'].value_counts()\n",
    "for location, count in gpe_entities.head(15).items():\n",
    "    print(f\"{location:25} â†’ {count:3} articles\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "visualization",\n",
   "metadata": {},\n",
   "source": [
    "## Step 7: Visualize Entity Distributions\n",
    "\n",
    "### Why visualization?\n",
    "Numbers are hard to interpret. Charts make patterns visible:\n",
    "- Which entity types dominate? (Is it mostly people or organizations?)\n",
    "- Are a few entities mentioned frequently, or is coverage distributed evenly?\n",
    "- What's the long tail? (Do 10 entities account for 50% of mentions?)\n",
    "- Which specific entities matter most in their category?\n",
    "\n",
    "### Understanding the charts\n",
    "- **Entity type distribution**: Breakdown of identified entity types (what did NER find?)\n",
    "- **Top people/organizations/locations**: Which specific entities matter most\n",
    "- **Skewed distributions**: A few entities mentioned many times is normal (power law distribution)\n",
    "\n",
    "### Interpretation tips\n",
    "If one entity dominates, ask why: Is it genuinely important or a data artifact? Does your dataset bias toward certain topics?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,\n",
   "id": "visualize-entities",\n",
   "metadata": {},\n",
   "outputs": [],\n",
    "source": [
    "# Visualize entity type distribution\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Entity type distribution\n",
    "entity_type_counts = entities_df['entity_type'].value_counts()\n",
    "axes[0, 0].bar(entity_type_counts.index, entity_type_counts.values, color='steelblue')\n",
    "axes[0, 0].set_title('Entity Types Distribution', fontsize=12, fontweight='bold')\n",
    "axes[0, 0].set_xlabel('Entity Type')\n",
    "axes[0, 0].set_ylabel('Count')\n",
    "axes[0, 0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Top 10 people\n",
    "person_top10 = person_entities.head(10)\n",
    "axes[0, 1].barh(person_top10.index, person_top10.values, color='coral')\n",
    "axes[0, 1].set_title('Top 10 People Mentioned', fontsize=12, fontweight='bold')\n",
    "axes[0, 1].set_xlabel('Mentions')\n",
    "\n",
    "# Top 10 organizations\n",
    "org_top10 = org_entities.head(10)\n",
    "axes[1, 0].barh(org_top10.index, org_top10.values, color='lightgreen')\n",
    "axes[1, 0].set_title('Top 10 Organizations', fontsize=12, fontweight='bold')\n",
    "axes[1, 0].set_xlabel('Mentions')\n",
    "\n",
    "# Top 10 locations\n",
    "gpe_top10 = gpe_entities.head(10)\n",
    "axes[1, 1].barh(gpe_top10.index, gpe_top10.values, color='lightyellow')\n",
    "axes[1, 1].set_title('Top 10 Locations', fontsize=12, fontweight='bold')\n",
    "axes[1, 1].set_xlabel('Mentions')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"âœ“ Visualizations complete\")"
   ]
  },
  {
   "cell_type": "markdown",\n",
   "id": "search-functionality",\n",
   "metadata": {},\n",
   "source": [
    "## Step 8: Build a Search Function\n",
    "\n",
    "### Real-world application\n",
    "News websites let users search by person, company, or location. This requires:\n",
    "1. Pre-extract all entities (we did this)\n",
    "2. Store them in a queryable database (we created the DataFrame)\n",
    "3. Build a search interface (this function)\n",
    "\n",
    "Without NER, searching requires keyword matching on raw text. With NER, you enable structured search:\n",
    "- \"Show articles mentioning a PERSON (not organizations)\"\n",
    "- \"Find articles about US locations only\"\n",
    "- \"Search for EVENTS, not organizations\"\n",
    "- \"Who are the organizations mentioned together with this person?\"\n",
    "\n",
    "### Scalability\n",
    "For small datasets (under 100k entities), this simple search is fast. For production:\n",
    "- **Elasticsearch**: Full-text entity search with advanced filters\n",
    "- **Vector embeddings**: Semantic search (\"companies similar to Apple\")\n",
    "- **Knowledge graphs**: Relationship queries (\"articles about Elon Musk AND Tesla together\")\n",
    "- **Caching**: Pre-compute frequent queries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,\n",
   "id": "search-function",\n",
   "metadata": {},\n",
   "outputs": [],\n",
    "source": [
    "def search_by_entity(entity_name, entity_type=None):\n",
    "    \"\"\"\n",
    "    Search articles mentioning a specific entity.\n",
    "    \n",
    "    Args:\n",
    "        entity_name: Name of the entity to search for\n",
    "        entity_type: Optional - filter by entity type (PERSON, ORG, GPE)\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame of articles mentioning the entity\n",
    "    \"\"\"\n",
    "    # Case-insensitive search\n",
    "    matches = entities_df[entities_df['entity_text'].str.contains(entity_name, case=False, na=False)]\n",
    "    \n",
    "    # Optional: filter by entity type\n",
    "    if entity_type:\n",
    "        matches = matches[matches['entity_type'] == entity_type]\n",
    "    \n",
    "    # Get unique articles\n",
    "    articles = matches[['article_title', 'article_date', 'entity_text', 'entity_type']].drop_duplicates()\n",
    "    \n",
    "    return articles\n",
    "\n",
    "# Example searches\n",
    "print(\"Search Example 1: Find all articles mentioning 'UK'\")\n",
    "print(\"=\" * 60)\n",
    "uk_articles = search_by_entity('UK')\n",
    "print(f\"Found {len(uk_articles)} articles\")\n",
    "print(uk_articles.head(5))\n",
    "\n",
    "print(\"\\n\\nSearch Example 2: Find all articles mentioning people named 'Boris'\")\n",
    "print(\"=\" * 60)\n",
    "boris_articles = search_by_entity('Boris', entity_type='PERSON')\n",
    "print(f\"Found {len(boris_articles)} articles\")\n",
    "print(boris_articles.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "handling-errors",\n",
   "metadata": {},\n",
   "source": [
    "## Step 9: Understanding NER Errors in Real Data\n",
    "\n",
    "### Why understanding errors matters\n",
    "NER is not perfect. Understanding failure modes prevents bad decisions:\n",
    "- If you delete duplicate entities without investigation, you might lose real data\n",
    "- If you trust all extracted entities uncritically, you'll have noisy analysis\n",
    "- If you don't know why something was missed, you can't improve your system\n",
    "\n",
    "### Three categories of errors\n",
    "**1. False positives** (marked as entity, but isn't)\n",
    "- Example: 'Python' tagged as ORG in 'I learned Python programming'\n",
    "- Impact: Inflates entity counts, adds noise to search results\n",
    "\n",
    "**2. False negatives** (is entity, but not marked)\n",
    "- Example: 'Apple' not tagged as ORG in lowercase tweet: 'apple announced new iphone'\n",
    "- Impact: Missing entities, incomplete coverage\n",
    "\n",
    "**3. Boundary errors** (wrong span)\n",
    "- Example: 'New York Times' split as 'New York' + 'Times' separately\n",
    "- Impact: Broken relationship data, incorrect normalization\n",
    "\n",
    "### Measuring error rates\n",
    "In production, you'd evaluate with metrics:\n",
    "- **Precision**: Of entities we found, how many are correct?\n",
    "- **Recall**: Of all true entities, how many did we find?\n",
    "- **F1 Score**: Balance between precision and recall (needed for trade-off evaluation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,\n",
   "id": "error-analysis",\n",
   "metadata": {},\n",
   "outputs": [],\n",
    "source": [
    "# Common NER errors found in real news\n",
    "print(\"Common NER Challenges in Real News:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Error 1: Inconsistent capitalization\n",
    "print(\"\\n1. CAPITALIZATION INCONSISTENCY:\")\n",
    "print(\"   'UK' vs 'uk' vs 'U.K.' all count as different entities\")\n",
    "inconsistent = entities_df[entities_df['entity_text'].str.lower().str.contains('uk')]\n",
    "print(f\"   Found {len(inconsistent.entity_text.unique())} variations of UK\")\n",
    "print(f\"   Examples: {list(inconsistent['entity_text'].unique()[:5])}\")\n",
    "\n",
    "# Error 2: Partial entity matches\n",
    "print(\"\\n2. PARTIAL ENTITY MATCHES:\")\n",
    "print(\"   'Harry' vs 'Harry Potter' may be tagged separately\")\n",
    "harry = entities_df[entities_df['entity_text'].str.contains('Harry', case=False)]\n",
    "print(f\"   Found {len(harry['entity_text'].unique())} Harry variations\")\n",
    "print(f\"   Examples: {list(harry['entity_text'].unique()[:5])}\")\n",
    "\n",
    "# Error 3: Common false positives\n",
    "print(\"\\n3. POTENTIAL FALSE POSITIVES:\")\n",
    "print(\"   Some entities might be wrongly classified\")\n",
    "# Check for single letters or very short entities (usually errors)\n",
    "short_entities = entities_df[entities_df['entity_text'].str.len() <= 2]\n",
    "print(f\"   Found {len(short_entities)} entities with 2 or fewer characters\")\n",
    "print(f\"   These may be errors: {short_entities['entity_text'].value_counts().head(5)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "entity-normalization",\n",
   "metadata": {},\n",
   "source": [
    "## Step 10: Entity Normalization (Fixing Common Errors)\n",
    "\n",
    "### The normalization problem\n",
    "Same entity, different forms:\n",
    "- 'USA', 'U.S.A.', 'United States' - all the same country\n",
    "- 'Apple Inc.', 'Apple Inc', 'Apple' - same company\n",
    "- 'Donald Trump', 'Trump', 'D. Trump' - same person\n",
    "\n",
    "Without normalization, these count as separate entities in your database. Your analysis becomes fragmented and unreliable.\n",
    "\n",
    "### Normalization strategies\n",
    "**1. Rule-based** (what we're doing here)\n",
    "- Replace known variations manually\n",
    "- Fast, explainable, requires domain knowledge\n",
    "\n",
    "**2. Statistical**\n",
    "- Group similar entities based on co-occurrence (if 'Apple' and 'Apple Inc.' appear in same articles)\n",
    "- Handles unknown variations automatically\n",
    "\n",
    "**3. Semantic**\n",
    "- Use word embeddings to find similar entity mentions\n",
    "- Very flexible, but computationally expensive\n",
    "\n",
    "**4. Entity linking**\n",
    "- Match to knowledge bases (Wikipedia, company databases)\n",
    "- Best approach, but requires external data\n",
    "\n",
    "### Trade-offs\n",
    "- **Too aggressive**: 'Trump' might incorrectly normalize to 'Donald Trump'\n",
    "- **Too conservative**: 'U.K.' doesn't normalize, fragmenting your data\n",
    "- **Best practice**: Start with rule-based for high-confidence cases, use human review for edge cases\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,\n",
   "id": "normalize-entities",\n",
   "metadata": {},\n",
   "outputs": [],\n",
    "source": [
    "# Function to normalize entities\n",
    "def normalize_entity(entity_text):\n",
    "    \"\"\"\n",
    "    Normalize entity text to handle common variations.\n",
    "    Examples: 'U.K.' â†’ 'UK', 'United States' â†’ 'US'\n",
    "    \"\"\"\n",
    "    # Remove extra whitespace\n",
    "    entity = entity_text.strip()\n",
    "    \n",
    "    # Common abbreviation normalizations\n",
    "    replacements = {\n",
    "        'U.K.': 'UK',\n",
    "        'U.S.': 'US',\n",
    "        'U.S.A.': 'USA',\n",
    "        'U.S.S.R': 'USSR',\n",
    "    }\n",
    "    \n",
    "    for old, new in replacements.items():\n",
    "        entity = entity.replace(old, new)\n",
    "    \n",
    "    return entity\n",
    "\n",
    "# Apply normalization\n",
    "entities_df['entity_normalized'] = entities_df['entity_text'].apply(normalize_entity)\n",
    "\n",
    "# Check normalization results\n",
    "print(\"Normalization Results:\")\n",
    "print(\"=\" * 60)\n",
    "normalized_counts = entities_df['entity_normalized'].value_counts()\n",
    "print(f\"\\nBefore normalization: {len(entities_df['entity_text'].unique())} unique entities\")\n",
    "print(f\"After normalization: {len(normalized_counts)} unique entities\")\n",
    "print(f\"\\nTop 20 normalized entities:\")\n",
    "print(normalized_counts.head(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "key-takeaways",\n",
   "metadata": {},\n",
   "source": [
    "## Key Takeaways\n",
    "\n",
    "**1. NER enables data extraction at scale**\n",
    "Processing 1,000+ articles manually would be impossible. NER automates it.\n",
    "\n",
    "**2. Real-world NER is messier than textbook examples**\n",
    "Abbreviations, multi-word entities, inconsistent capitalization, domain-specific termsâ€”all require handling in production systems.\n",
    "\n",
    "**3. Entity types are meaningful separators**\n",
    "Separating people from companies from locations enables different kinds of analysis and enables type-specific search.\n",
    "\n",
    "**4. No model is perfectâ€”design for errors**\n",
    "Expect false positives (wrong entities) and false negatives (missed entities). Build systems that gracefully handle errors.\n",
    "\n",
    "**5. Normalization is essential for usable databases**\n",
    "Without consolidating entity variations, your database becomes fragmented and unreliable.\n",
    "\n",
    "**6. Frequency analysis reveals structure**\n",
    "What entities are mentioned most? That tells you what the data is fundamentally about.\n",
    "\n",
    "**7. Structure enables search and analysis**\n",
    "Type-based, entity-based search is only possible because we extracted and categorized entities.\n",
    "\n",
    "**8. Visualization makes patterns visible**\n",
    "Charts reveal distribution patterns and outliers that raw numbers hide.\n",
    "\n",
    "## Next Steps:\n",
    "- **Entity Linking**: Connect entities to Wikipedia or company databases for disambiguation\n",
    "- **Relation Extraction**: Find relationships between entities (\"Company X was founded by Person Y\")\n",
    "- **Temporal Analysis**: Track how entity frequencies change over time (trend detection)\n",
    "- **Knowledge Graph**: Build a network of entities and relationships\n",
    "- **Custom NER**: Train domain-specific models (stock tickers, medical terms, legal entities)\n",
    "- **Error Metrics**: Calculate precision/recall on a manually annotated test set\n"
   ]
  },\n",
   "cell_type": "markdown",\n",
   "id": "practice",\n",
   "metadata": {},\n",
   "source": [
    "## Practice Exercises\n",
    "\n",
    "Try these on your own:"
   ]
  },\n",
   "cell_type": "code",\n",
   "execution_count": null,\n",
   "id": "practice-1",\n",
   "metadata": {},\n",
   "outputs": [],\n",
    "source": [
    "# Exercise 1: Find articles mentioning a specific company\n",
    "# Modify and search for your company of interest\n",
    "company = 'Apple'  # Change this to search for different companies\n",
    "company_articles = search_by_entity(company, entity_type='ORG')\n",
    "print(f\"Articles mentioning {company}:\")\n",
    "print(company_articles)"
   ]
  },\n",
   "cell_type": "code",\n",
   "execution_count": null,\n",
   "id": "practice-2",\n",
   "metadata": {},\n",
   "outputs": [],\n",
    "source": [
    "# Exercise 2: Find articles about a specific country/region\n",
    "location = 'China'  # Change to search different locations\n",
    "location_articles = search_by_entity(location, entity_type='GPE')\n",
    "print(f\"\\nArticles about {location}:\")\n",
    "print(f\"Total: {len(location_articles)} articles\")\n",
    "print(location_articles.head(10))"
   ]
  },\n",
   "cell_type": "code",\n",
   "execution_count": null,\n",
   "id": "practice-3",\n",
   "metadata": {},\n",
   "outputs": [],\n",
    "source": [
    "# Exercise 3: Create a co-occurrence analysis\n",
    "# Find entities that appear together in articles\n",
    "# Example: People and companies mentioned in the same article\n",
    "\n",
    "def find_entity_pairs(article_title, entity_type1='PERSON', entity_type2='ORG'):\n",
    "    \"\"\"\n",
    "    Find pairs of entities that appear in the same article.\n",
    "    \"\"\"\n",
    "    article_entities = entities_df[entities_df['article_title'] == article_title]\n",
    "    \n",
    "    type1_entities = article_entities[article_entities['entity_type'] == entity_type1]['entity_text'].unique()\n",
    "    type2_entities = article_entities[article_entities['entity_type'] == entity_type2]['entity_text'].unique()\n",
    "    \n",
    "    return type1_entities, type2_entities\n",
    "\n",
    "# Test on first article\n",
    "first_article = bbc_news.iloc[0]['title']\n",
    "people, orgs = find_entity_pairs(first_article)\n",
    "print(f\"Article: {first_article}\")\n",
    "print(f\"People mentioned: {list(people)}\")\n",
    "print(f\"Organizations mentioned: {list(orgs)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_course_env",
   "language": "python",
   "name": "nlp_course_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
