{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "anchor-section",
   "metadata": {},
   "source": [
    "# ðŸ“Œ Topic: N-Grams\n",
    "\n",
    "### What you will learn\n",
    "- What N-grams are and why they matter\n",
    "- How to construct unigrams, bigrams, and trigrams\n",
    "- Understanding word relationships and context\n",
    "- Frequency analysis of N-grams\n",
    "- Visualizing N-gram distributions\n",
    "- Real-world applications\n",
    "\n",
    "### Why this matters\n",
    "N-grams capture **sequential relationships between words**. While individual words (unigrams) lose context, bigrams and trigrams capture natural language patterns. N-grams are essential for language modeling, auto-complete systems, machine translation, and many other NLP tasks. They're also one of the few text features that \"vanilla\" machine learning models can use effectively.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": [
    "## What are N-Grams?\n",
    "\n",
    "An **N-gram** is a contiguous sequence of **N tokens** (usually words) from a text. N-grams capture word context and relationships.\n",
    "\n",
    "### Examples (from \"I love you\"):\n",
    "\n",
    "| N | Type | Tokens |\n",
    "|---|------|--------|\n",
    "| 1 | **Unigram** | \"I\", \"love\", \"you\" |\n",
    "| 2 | **Bigram** | (\"I\", \"love\"), (\"love\", \"you\") |\n",
    "| 3 | **Trigram** | (\"I\", \"love\", \"you\") |\n",
    "| 4 | **4-gram** | N/A (text too short) |\n",
    "\n",
    "### Why N-grams matter:\n",
    "\n",
    "**Unigrams alone lose context:**\n",
    "- Text: \"dog bites man\" vs. \"man bites dog\"\n",
    "- Unigrams: {\"dog\", \"bites\", \"man\"} â€” Same for both! (Wrong!)\n",
    "\n",
    "**Bigrams capture relationships:**\n",
    "- \"dog bites man\": (\"dog\", \"bites\"), (\"bites\", \"man\")\n",
    "- \"man bites dog\": (\"man\", \"bites\"), (\"bites\", \"dog\")\n",
    "- Now they're different! (Correct!)\n",
    "\n",
    "### Common use cases:\n",
    "1. **Language modeling**: Predicting next word (auto-complete)\n",
    "2. **Machine translation**: Identifying phrases vs. individual words\n",
    "3. **Spam detection**: Bigrams identify common spam phrases\n",
    "4. **Text classification**: Capture important phrase patterns\n",
    "5. **Information retrieval**: Better query matching with phrase awareness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1553a2b-6338-4e9b-9246-e1677861b8cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# N-grams help analyze relationships between neighboring words\n",
    "# They capture sequential patterns that single words miss\n",
    "# Example: \"New York\" is different from \"new\" + \"york\" separately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f18e7d74-bee1-4620-825a-8ec368a84331",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definition: An N-gram is a sequence of N consecutive tokens\n",
    "# N=1: Unigram (single word)\n",
    "# N=2: Bigram (word pair)\n",
    "# N=3: Trigram (word triple)\n",
    "# Higher N values capture longer phrases but are more sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80133514-5074-4fad-9c2a-1560479c9815",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visual example of how N-grams are extracted from text\n",
    "# Input: \"I love you\"\n",
    "# Unigrams (N=1): [\"I\"] [\"love\"] [\"you\"]\n",
    "# Bigrams (N=2): [\"I\", \"love\"] [\"love\", \"you\"]\n",
    "# Trigrams (N=3): [\"I\", \"love\", \"you\"]\n",
    "\n",
    "ngram_example = '''N=1: Unigram \"I\" \"love\" \"you\" \n",
    "N=2: Bigram (\"I\", \"love\") (\"love\", \"you\")\n",
    "N=3: Trigram (\"I\", \"love\", \"you\")'''\n",
    "\n",
    "print(ngram_example)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup",
   "metadata": {},
   "source": [
    "## Setup: Import Libraries\n",
    "\n",
    "We'll use:\n",
    "- **NLTK**: For generating N-grams\n",
    "- **Pandas**: For counting frequencies and creating DataFrames\n",
    "- **Matplotlib**: For visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c555b106-10d9-4757-aad4-c6d77425e9f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries for N-gram analysis\n",
    "import nltk  # Natural Language Toolkit for NLP operations\n",
    "import pandas as pd  # Data manipulation and frequency counting\n",
    "import matplotlib.pyplot as plt  # Data visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "token-preparation",
   "metadata": {},
   "source": [
    "## Step 1: Prepare Tokenized Text\n",
    "\n",
    "N-grams work on **tokenized text** (pre-split into words). Always preprocess your text first:\n",
    "- Convert to lowercase\n",
    "- Remove punctuation (usually)\n",
    "- Tokenize into words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "601e4497-2424-4320-b130-fcc0ecfd2cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example text about AI and machine learning (already tokenized and lowercased)\n",
    "# In real work, you'd tokenize raw text using nltk.word_tokenize() first\n",
    "tokens = [\n",
    "    'the', 'rise', 'of', 'artificial', 'intelligence', 'has', 'led', 'to', 'significant',\n",
    "    'advancements', 'in', 'natural', 'language', 'processing', 'computer', 'vision', 'and',\n",
    "    'other', 'fields', 'machine', 'learning', 'algorithms', 'are', 'becoming', 'more',\n",
    "    'sophisticated', 'enabling', 'computers', 'to', 'perform', 'complex', 'tasks', 'that',\n",
    "    'were', 'once', 'thought', 'to', 'be', 'the', 'exclusive', 'domain', 'of', 'humans',\n",
    "    'with', 'the', 'advent', 'of', 'deep', 'learning', 'neural', 'networks', 'have',\n",
    "    'become', 'even', 'more', 'powerful', 'capable', 'of', 'processing', 'vast', 'amounts',\n",
    "    'of', 'data', 'and', 'learning', 'from', 'it', 'in', 'ways', 'that', 'were', 'not',\n",
    "    'possible', 'before', 'as', 'a', 'result', 'ai', 'is', 'increasingly', 'being', 'used',\n",
    "    'in', 'a', 'wide', 'range', 'of', 'industries', 'from', 'healthcare', 'to', 'finance',\n",
    "    'to', 'transportation', 'and', 'its', 'impact', 'is', 'only', 'set', 'to', 'grow', 'in',\n",
    "    'the', 'years', 'to', 'come'\n",
    "]\n",
    "\n",
    "print(f\"Total tokens: {len(tokens)}\")\n",
    "print(f\"First 10 tokens: {tokens[:10]}\")\n",
    "print(f\"Last 5 tokens: {tokens[-5:]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unigram-section",
   "metadata": {},
   "source": [
    "## Step 2: Generating Unigrams\n",
    "\n",
    "Unigrams are the simplest: each individual word becomes a token. We'll count the frequency of each unigram.\n",
    "\n",
    "### How it works:\n",
    "```python\n",
    "nltk.ngrams(tokens, 1)  # Returns single-word tuples\n",
    "# Output: ('the',), ('rise',), ('of',), ...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unigram-generation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate unigrams (individual words) from token list\n",
    "# nltk.ngrams(tokens, 1) creates a sliding window of size 1\n",
    "# Each unigram is a tuple containing one word: ('the',), ('rise',), etc.\n",
    "unigrams = pd.Series(nltk.ngrams(tokens, 1)).value_counts()\n",
    "\n",
    "# Display frequency counts\n",
    "print(\"Top 10 Most Common Unigrams:\")\n",
    "print(\"=\" * 40)\n",
    "print(unigrams.head(10))\n",
    "\n",
    "print(\"\\nLeast Common Unigrams:\")\n",
    "print(\"=\" * 40)\n",
    "print(unigrams.tail(10))\n",
    "\n",
    "print(f\"\\nTotal unique unigrams: {len(unigrams)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bigram-section",
   "metadata": {},
   "source": [
    "## Step 3: Generating Bigrams\n",
    "\n",
    "Bigrams capture word pairs and are extremely useful for language modeling and phrase detection.\n",
    "\n",
    "### Example (from our text):\n",
    "- Most common bigrams likely include: (\"the\", \"rise\"), (\"of\", \"artificial\"), etc.\n",
    "- These bigrams represent meaningful phrases and relationships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bigram-generation",\n",
   "metadata": {},\n",
   "outputs": [],\n",
    "source": [
    "# Generate bigrams (word pairs) from token list\n",
    "# nltk.ngrams(tokens, 2) creates a sliding window of size 2\n",
    "# Each bigram is a tuple of two consecutive words\n",
    "# Example: ('the', 'rise'), ('rise', 'of'), ('of', 'artificial'), ...\n",
    "bigrams = pd.Series(nltk.ngrams(tokens, 2)).value_counts()\n",
    "\n",
    "# Display frequency counts\n",
    "print(\"Top 10 Most Common Bigrams:\")\n",
    "print(\"=\" * 50)\n",
    "print(bigrams.head(10))\n",
    "\n",
    "print(f\"\\nTotal unique bigrams: {len(bigrams)}\")\n",
    "\n",
    "# Insight: Bigrams reveal common phrases\n",
    "# 'the' appears in many bigrams because it's the most common unigram"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "trigram-section",
   "metadata": {},
   "source": [
    "## Step 4: Generating Trigrams\n",
    "\n",
    "Trigrams capture three-word sequences and are useful for more specific phrase detection.\n",
    "\n",
    "**Note**: As N increases, data becomes **sparser**:\n",
    "- Unigrams: May appear 5-10 times each\n",
    "- Bigrams: May appear 1-3 times each\n",
    "- Trigrams: Often appear only once in small texts\n",
    "\n",
    "This is the **data sparsity problem** in NLP. More context = less coverage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,\n",
   "id": "trigram-generation",\n",
   "metadata": {},\n",
   "outputs": [],\n",
    "source": [
    "# Generate trigrams (three-word sequences) from token list\n",
    "# nltk.ngrams(tokens, 3) creates a sliding window of size 3\n",
    "# Example: ('the', 'rise', 'of'), ('rise', 'of', 'artificial'), ...\n",
    "trigrams = pd.Series(nltk.ngrams(tokens, 3)).value_counts()\n",
    "\n",
    "# Display frequency counts\n",
    "print(\"Top 10 Most Common Trigrams:\")\n",
    "print(\"=\" * 60)\n",
    "print(trigrams.head(10))\n",
    "\n",
    "print(f\"\\nTotal unique trigrams: {len(trigrams)}\")\n",
    "\n",
    "# Observation: Notice sparsity increase\n",
    "# Most trigrams appear only 1-2 times\n",
    "# This is expected with small corpora"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comparison",\n",
   "metadata": {},
   "source": [
    "## Comparing N-gram Levels\n",
    "\n",
    "Let's see how statistics change as N increases:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,\n",
   "id": "comparison-code",\n",
   "metadata": {},\n",
   "outputs": [],\n",
    "source": [
    "# Analyze statistics across different N-gram levels\n",
    "print(\"N-gram Statistics Comparison:\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"{'Metric':<30} | {'Unigrams':<15} | {'Bigrams':<15} | {'Trigrams':<15}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "stats = {\n",
    "    'Total N-grams': [len(list(nltk.ngrams(tokens, 1))), \n",
    "                      len(list(nltk.ngrams(tokens, 2))), \n",
    "                      len(list(nltk.ngrams(tokens, 3)))],\n",
    "    'Unique N-grams': [len(unigrams), len(bigrams), len(trigrams)],\n",
    "    'Most common freq': [unigrams.max(), bigrams.max(), trigrams.max()],\n",
    "    'Avg frequency': [unigrams.mean(), bigrams.mean(), trigrams.mean()],\n",
    "}\n",
    "\n",
    "for metric, values in stats.items():\n",
    "    print(f\"{metric:<30} | {values[0]:<15} | {values[1]:<15} | {values[2]:<15}\")\n",
    "\n",
    "print(\"\\nâœ“ Key insight: As N increases, unique N-grams increase, frequency decreases\")\n",
    "print(\"âœ“ Sparsity problem: Trigrams appear fewer times than bigrams or unigrams\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "visualization",\n",
   "metadata": {},
   "source": [
    "## Step 5: Visualization\n",
    "\n",
    "Let's visualize the frequency distribution of unigrams as a bar chart. This helps understand which words dominate the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,\n",
   "id": "77201f00-c648-4799-b6c2-36255c9e8982",\n",
   "metadata": {},\n",
   "outputs": [],\n",
    "source": [
    "# Create a bar chart of the 10 most frequent unigrams\n",
    "# This visualization helps identify dominant words in the text\n",
    "unigrams_top = unigrams[:10].sort_values()\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "# Create horizontal bar chart (easier to read word names)\n",
    "unigrams_top.plot.barh(color=\"steelblue\", width=0.7)\n",
    "plt.xlabel(\"Frequency\", fontsize=12)\n",
    "plt.ylabel(\"Unigram\", fontsize=12)\n",
    "plt.title(\"Top 10 Most Common Unigrams\", fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Insight: 'to', 'of', 'the' are most common\n",
    "# These are stop words â€” they appear frequently but carry less semantic meaning\n",
    "# In some NLP tasks, we remove these for better signal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bigram-visualization",\n",
   "metadata": {},\n",
   "source": [
    "## Visualizing Bigrams\n",
    "\n",
    "Now let's look at the most common bigrams (word pairs), which reveal actual phrases in the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,\n",
   "id": "bigram-viz",\n",
   "metadata": {},\n",
   "outputs": [],\n",
    "source": [
    "# Create a bar chart of the top bigrams\n",
    "# Bigrams show meaningful phrases, not just common words\n",
    "bigrams_top = bigrams[:10].sort_values()\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "# Convert tuples to strings for better readability\n",
    "labels = [' + '.join(bg) for bg in bigrams_top.index]\n",
    "plt.barh(labels, bigrams_top.values, color=\"coral\", width=0.7)\n",
    "plt.xlabel(\"Frequency\", fontsize=12)\n",
    "plt.ylabel(\"Bigram\", fontsize=12)\n",
    "plt.title(\"Top 10 Most Common Bigrams\", fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Insight: Bigrams show actual phrases\n",
    "# 'of' combines with many words: ('of', 'x'), ('x', 'of')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "applications",\n",
   "metadata": {},
   "source": [
    "## Real-World Applications of N-Grams\n",
    "\n",
    "### 1. **Auto-complete / Next Word Prediction**\n",
    "```\n",
    "User types: \"machine\"\n",
    "Bigrams show: \"machine learning\" appears 3 times\n",
    "System suggests: \"learning\" as next word\n",
    "```\n",
    "\n",
    "### 2. **Spell Checking**\n",
    "```\n",
    "Input: \"natural languge processing\"\n",
    "Trigram: ('natural', 'language', 'processing') is valid\n",
    "Bigram: ('natural', 'languge') never appears\n",
    "System flags: \"languge\" as misspelling\n",
    "```\n",
    "\n",
    "### 3. **Machine Translation**\n",
    "```\n",
    "English phrase: \"good morning\"\n",
    "N-grams capture common collocations\n",
    "Decoder chooses best translation preserving phrase structure\n",
    "```\n",
    "\n",
    "### 4. **Spam Detection**\n",
    "```\n",
    "Spam bigrams: (\"click\", \"here\"), (\"buy\", \"now\"), (\"limited\", \"offer\")\n",
    "Ham bigrams: (\"looking\", \"forward\"), (\"thanks\", \"for\")\n",
    "Classifier uses N-gram frequencies to detect spam\n",
    "```\n",
    "\n",
    "### 5. **Plagiarism Detection**\n",
    "```\n",
    "Document A: \"natural language processing is\"\n",
    "Document B: \"language processing is natural\" (same words, different order)\n",
    "Bigrams different â†’ Not plagiarized (good detection!)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "smoothing",\n",
   "metadata": {},
   "source": [
    "## Advanced: Data Sparsity and Smoothing\n",
    "\n",
    "The **sparsity problem**: Most N-grams appear 0-1 times. Unfrequent N-grams break models.\n",
    "\n",
    "### Solution: Smoothing Techniques\n",
    "\n",
    "1. **Add-One (Laplace) Smoothing**\n",
    "```python\n",
    "P(bigram) = (count + 1) / (total_bigrams + vocabulary_size)\n",
    "# Never assigns 0 probability to unseen bigrams\n",
    "```\n",
    "\n",
    "2. **Backoff**: If trigram unseen, fall back to bigram\n",
    "```\n",
    "P(\"the cat sat\") unseen?\n",
    "â†’ Use P(\"cat sat\")\n",
    "â†’ Use P(\"sat\")\n",
    "```\n",
    "\n",
    "3. **Interpolation**: Combine multiple N-gram levels\n",
    "```\n",
    "P(bigram) = 0.5 * P(bigram) + 0.3 * P(unigram_1) + 0.2 * P(unigram_2)\n",
    "```\n",
    "\n",
    "Modern neural models (like transformers) handle sparsity automatically."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "key-takeaways",\n",
   "metadata": {},\n",
   "source": [
    "## Key Takeaways\n",
    "\n",
    "1. **N-grams capture context**: Unigrams alone lose word order information\n",
    "2. **Increasing N = more sparsity**: Bigrams appear more than trigrams\n",
    "3. **NLTK makes N-grams easy**: `nltk.ngrams(tokens, n)` is your friend\n",
    "4. **Frequency analysis reveals patterns**: Most common N-grams are often stop words\n",
    "5. **Visualization helps understanding**: Charts show N-gram distributions clearly\n",
    "6. **Practical applications**: Auto-complete, spell check, plagiarism detection, spam filtering\n",
    "7. **Smoothing solves sparsity**: Handle unseen N-grams gracefully\n",
    "\n",
    "## Next Steps:\n",
    "- Learn about **stopword removal** (remove \"the\", \"a\", \"and\" for cleaner analysis)\n",
    "- Explore **TF-IDF** and **bag-of-words** models (use N-grams as features)\n",
    "- Study **language models** (foundation for transformers like BERT, GPT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "practice",\n",
   "metadata": {},\n",
   "source": [
    "## Practice Exercise\n",
    "\n",
    "Try these exercises on your own:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,\n",
   "id": "practice-1",\n",
   "metadata": {},\n",
   "outputs": [],\n",
    "source": [
    "# Exercise 1: Generate 4-grams and see how sparse they are\n",
    "fourgrams = pd.Series(nltk.ngrams(tokens, 4)).value_counts()\n",
    "print(f\"Total 4-grams: {len(list(nltk.ngrams(tokens, 4)))}\")\n",
    "print(f\"Unique 4-grams: {len(fourgrams)}\")\n",
    "print(f\"\\nMost common 4-grams:\")\n",
    "print(fourgrams.head())"
   ]
  },\n",
   "cell_type": "code",\n",
   "execution_count": null,\n",
   "id": "practice-2",\n",
   "metadata": {},\n",
   "outputs": [],\n",
    "source": [
    "# Exercise 2: Find all bigrams containing \"learning\"\n",
    "learning_bigrams = [bg for bg in bigrams.index if 'learning' in bg]\n",
    "print(f\"Bigrams containing 'learning':\")\n",
    "for bg in learning_bigrams:\n",
    "    print(f\"  {bg}: {bigrams[bg]} times\")"
   ]
  },\n",
   "cell_type": "code",\n",
   "execution_count": null,\n",
   "id": "practice-3",\n",
   "metadata": {},\n",
   "outputs": [],\n",
    "source": [
    "# Exercise 3: Create a custom sentence and analyze its N-grams\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "custom_text = \"natural language processing is fascinating\"\n",
    "custom_tokens = word_tokenize(custom_text.lower())\n",
    "\n",
    "custom_unigrams = pd.Series(nltk.ngrams(custom_tokens, 1)).value_counts()\n",
    "custom_bigrams = pd.Series(nltk.ngrams(custom_tokens, 2)).value_counts()\n",
    "\n",
    "print(f\"Text: {custom_text}\")\n",
    "print(f\"\\nUnigrams: {custom_unigrams.to_dict()}\")\n",
    "print(f\"Bigrams: {custom_bigrams.to_dict()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_course_env",
   "language": "python",
   "name": "nlp_course_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
