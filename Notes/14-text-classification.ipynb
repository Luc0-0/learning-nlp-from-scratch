{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "anchor-section",
            "metadata": {},
            "source": [
                "# ðŸ“Œ Topic: Text Classification\n",
                "\n",
                "### What you will learn\n",
                "- How to train supervised learning models on text data\n",
                "- Comparison of three popular algorithms: Logistic Regression, Naive Bayes, and SVM\n",
                "- The standard ML pipeline: Bag of Words -> Splitting -> Training -> Evaluation\n",
                "- How to interpret classification performance (Accuracy, Precision, Recall)\n",
                "\n",
                "### Why this matters\n",
                "Text classification is the backbone of many real-world applications: spam filters, sentiment analyzers (like detecting hate speech), and automated customer support sorting. Understanding which algorithm to use and how to evaluate it is key to building reliable NLP systems.\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "ml-workflow",
            "metadata": {},
            "source": [
                "## The Workflow\n",
                "\n",
                "1.  **Vectorization**: Transform text into numbers (we'll use Bag of Words here).\n",
                "2.  **Dataset Splitting**: Separating data into a **Training Set** (to teach the model) and a **Testing Set** (to check its homework).\n",
                "3.  **Model Training**: Letting the algorithms find patterns between word counts and labels.\n",
                "4.  **Evaluation**: Measuring how many labels the model got right on unseen data."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "6441d7aa-6d3e-4c8b-bdad-3b6f55bffd1b",
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd \n",
                "from sklearn.feature_extraction.text import CountVectorizer\n",
                "from sklearn.model_selection import train_test_split\n",
                "from sklearn.linear_model import LogisticRegression\n",
                "from sklearn.naive_bayes import MultinomialNB\n",
                "from sklearn.linear_model import SGDClassifier\n",
                "from sklearn.metrics import accuracy_score, classification_report\n",
                "\n",
                "# Create a toy sentiment dataset\n",
                "data = pd.DataFrame(\n",
                "    [\n",
                "        (\"I love spending time with my friends and family\", \"positive\"),\n",
                "        (\"That was the best meal I've ever had in my life\", \"positive\"),\n",
                "        (\"I feel so grateful for everything I have in my life\", \"positive\"),\n",
                "        (\"I received a promotion at work and I couldn't be happier\", \"positive\"),\n",
                "        (\"Watching a beautiful sunset always fills me with joy\", \"positive\"),\n",
                "        (\"My partner surprised me with a thoughtful gift and it made my day\", \"positive\"),\n",
                "        (\"I am so proud of my daughter for graduating with honors\", \"positive\"),\n",
                "        (\"Listening to my favorite music always puts me in a good mood\", \"positive\"),\n",
                "        (\"I love the feeling of accomplishment after completing a challenging task\", \"positive\"),\n",
                "        (\"I am excited to go on vacation next week\", \"positive\"),\n",
                "        (\"I feel so overwhelmed with work and responsibilities\", \"negative\"),\n",
                "        (\"The traffic during my commute is always so frustrating\", \"negative\"),\n",
                "        (\"I received a parking ticket and it ruined my day\", \"negative\"),\n",
                "        (\"I got into an argument with my partner and we're not speaking\", \"negative\"),\n",
                "        (\"I have a headache and I feel terrible\", \"negative\"),\n",
                "        (\"I received a rejection letter for the job I really wanted\", \"negative\"),\n",
                "        (\"My car broke down and it's going to be expensive to fix\", \"negative\"),\n",
                "        (\"I'm feeling sad because I miss my friends who live far away\", \"negative\"),\n",
                "        (\"I'm frustrated because I can't seem to make progress on my project\", \"negative\"),\n",
                "        (\"I'm disappointed because my team lost the game\", \"negative\"),\n",
                "    ],\n",
                "    columns=[\"text\", \"sentiment\"]\n",
                ")\n",
                "\n",
                "# Shuffle the data for fair training\n",
                "data = data.sample(frac=1, random_state=42).reset_index(drop=True)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "preprocessing",
            "metadata": {},
            "source": [
                "## Step 1: Preprocessing and Vectorizing\n",
                "\n",
                "We convert our text column into numerical features using `CountVectorizer`."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "fdbda78f-2e60-4f1b-bcae-883f1726e860",
            "metadata": {},
            "outputs": [],
            "source": [
                "x = data['text']\n",
                "y = data['sentiment']\n",
                "\n",
                "# Transform text into Bag of Words representation\n",
                "vectorizer = CountVectorizer()\n",
                "x_bow = vectorizer.fit_transform(x)\n",
                "\n",
                "# Split the data: 70% for training, 30% for testing\n",
                "x_train, x_test, y_train, y_test = train_test_split(x_bow, y, test_size=0.3, random_state=7)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "logistic-regression",
            "metadata": {},
            "source": [
                "## Step 2: Logistic Regression\n",
                "\n",
                "**Logistic Regression** predicts probabilities for binary classes. It works by finding the weights for each word that best separate positive from negative samples."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "e6aabd45-2695-46f1-b383-c26166cb40f1",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Initialize and train\n",
                "lr_model = LogisticRegression()\n",
                "lr_model.fit(x_train, y_train)\n",
                "\n",
                "# Prediction and evaluation\n",
                "y_pred_lr = lr_model.predict(x_test)\n",
                "print(f\"Logistic Regression Accuracy: {accuracy_score(y_test, y_pred_lr):.2f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "naive-bayes",
            "metadata": {},
            "source": [
                "## Step 3: Multinomial Naive Bayes\n",
                "\n",
                "**Naive Bayes** is based on Bayes' Theorem. It's \"naive\" because it assumes every word is independent of others (which we know isn't true), but it's incredibly fast and often performs surprisingly well on text data."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "b83095f5-bf31-4eaa-ba16-b88bb2f856bd",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Train Naive Bayes\n",
                "nb_model = MultinomialNB()\n",
                "nb_model.fit(x_train.toarray(), y_train)\n",
                "\n",
                "# Predict\n",
                "y_pred_nb = nb_model.predict(x_test.toarray())\n",
                "print(f\"Naive Bayes Accuracy: {accuracy_score(y_test, y_pred_nb):.2f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "svm",
            "metadata": {},
            "source": [
                "## Step 4: Support Vector Machine (SVM)\n",
                "\n",
                "**SVM** tries to find the widest possible \"road\" or boundary between the two classes. It is very effective in high-dimensional spaces (like text datasets where every word is a dimension)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "6190f3a8-98e0-41f8-912a-7036c8b815cf",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Using SGDClassifier as an efficient implementation of linear SVM\n",
                "svm_model = SGDClassifier()\n",
                "svm_model.fit(x_train, y_train)\n",
                "\n",
                "# Predict\n",
                "y_pred_svm = svm_model.predict(x_test)\n",
                "print(f\"SVM Accuracy: {accuracy_score(y_test, y_pred_svm):.2f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "comparison",
            "metadata": {},
            "source": [
                "## Comparing the Results\n",
                "\n",
                "A single accuracy number doesn't tell the whole story. Let's look at the detailed breakdown."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "65532540-d94b-471e-b663-1e2a531583f3",
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"Detailed Report (Logistic Regression):\")\n",
                "print(classification_report(y_test, y_pred_lr, zero_division=0))"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "key-takeaways",
            "metadata": {},
            "source": [
                "## Key Takeaways\n",
                "\n",
                "1.  **Fast Baseline**: Naive Bayes is usually the fastest to train and serves as a great starting reference.\n",
                "2.  **High Accuracy**: Logistic Regression and SVM tend to be more accurate on larger datasets where word interactions matter more.\n",
                "3.  **Data Quality**: Features (BoW, TF-IDF) often matter just as much as the algorithm itself.\n",
                "\n",
                "## Next steps:\n",
                "- Try adding **Stopword Removal** to see if accuracy improves.\n",
                "- Experiment with **Hyperparameter Tuning** (e.g., changing the `C` parameter in Logistic Regression).\n",
                "- Learn about **Dimensionality Reduction** in the next notebook to handle very large vocabularies."
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "nlp_course_env",
            "language": "python",
            "name": "nlp_course_env"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.11.14"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}