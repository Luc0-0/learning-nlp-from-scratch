{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "anchor-section",
            "metadata": {},
            "source": [
                "# ðŸ“Œ Topic: Bag of Words (BoW) Model\n",
                "\n",
                "### What you will learn\n",
                "- What the Bag of Words model is and how it represents text numerically\n",
                "- How to use `CountVectorizer` to create document-term matrices\n",
                "- Understanding vocabulary building and word frequencies\n",
                "- Limitations of the BoW approach\n",
                "\n",
                "### Why this matters\n",
                "Machine learning models can't process raw text directly. They need numbers. The **Bag of Words** model is one of the simplest and most common ways to convert text into numerical vectors that models can understand. It sets the stage for more advanced techniques like TF-IDF and word embeddings.\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "intro",
            "metadata": {},
            "source": [
                "## What is Bag of Words?\n",
                "\n",
                "The **Bag of Words** model represents text by ignoring grammar and word order, focusing only on the presence and frequency of words. Imagine throwing all the words from a document into a literal \"bag\"â€”you know which words are inside and how many of each there are, but you've lost the structure of the sentences.\n",
                "\n",
                "### How it works:\n",
                "1. **Tokenization**: Breaking the text into individual words.\n",
                "2. **Vocabulary Building**: Collecting every unique word across all documents in your dataset.\n",
                "3. **Vectorization**: Representing each document as a row where each column corresponds to a word in the vocabulary, and the values are the counts of those words in the document."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "273f14af-3cb0-4807-bfba-878a036e4576",
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "from sklearn.feature_extraction.text import CountVectorizer\n",
                "\n",
                "# Sample dataset of sentences for demonstration\n",
                "data = [\n",
                "    \"I love learning natural language processing\",\n",
                "    \"Natural language processing is fun to learn\",\n",
                "    \"I enjoy learning new things in machine learning\",\n",
                "    \"Machine learning helps computers understand language\",\n",
                "    \"I love using language models for learning\"\n",
                "]"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "vectorization-step",
            "metadata": {},
            "source": [
                "## Step 1: Initialize the CountVectorizer\n",
                "\n",
                "Scikit-learn provides `CountVectorizer` to handle the heavy lifting. It automatically tokenizes the text, builds the vocabulary, and creates the frequency counts."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "7a147528-e905-4e84-bc41-b9f9b39045dc",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Initialize the vectorizer\n",
                "countvec = CountVectorizer()\n",
                "\n",
                "# Fit and transform the data\n",
                "# 'Fit' learns the vocabulary; 'Transform' creates the document-term matrix\n",
                "countvec_fit = countvec.fit_transform(data)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "display-results",
            "metadata": {},
            "source": [
                "## Step 2: Inspecting the Matrix\n",
                "\n",
                "Let's convert the sparse matrix into a readable DataFrame so we can see which words appear where."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "3b4634f1-0a14-4ee6-ae73-c9baf3da7227",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create a DataFrame with column names matching the vocabulary terms\n",
                "bag_of_words = pd.DataFrame(countvec_fit.toarray(), columns=countvec.get_feature_names_out())\n",
                "\n",
                "# Let's add the original sentences for easier comparison\n",
                "bag_of_words['ORIGINAL_SENTENCE'] = data\n",
                "\n",
                "# Display the result\n",
                "bag_of_words"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "limitations",
            "metadata": {},
            "source": [
                "## Key Takeaways\n",
                "\n",
                "1. **Fixed Size**: Every document is represented by a vector of the same length (the size of the total vocabulary).\n",
                "2. **Simple Counts**: Values indicate how many times a word appears in that specific document.\n",
                "3. **Loss of Context**: Position and meaning (semantics) are discarded. \"Dog bites man\" and \"Man bites dog\" would look identical in basic BoW.\n",
                "4. **Sparsity**: Most cells in the matrix will be zero, as most documents only use a small fraction of the total vocabulary.\n",
                "\n",
                "## Next steps:\n",
                "- Explore **Stopword Removal** to exclude common words like \"is\", \"to\", \"for\" that don't add much meaning.\n",
                "- Move on to **TF-IDF Vectorization** to see how we can weight words by importance rather than just raw frequency."
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "nlp_course_env",
            "language": "python",
            "name": "nlp_course_env"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.11.14"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}