{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "anchor-section",
   "metadata": {},
   "source": [
    "# üìå Topic: Tokenization\n",
    "\n",
    "### What you will learn\n",
    "- What tokenization is and why it's fundamental to NLP\n",
    "- How to split text into sentences (sentence tokenization)\n",
    "- How to split text into individual words (word tokenization)\n",
    "- Common edge cases (contractions, punctuation, possessives)\n",
    "- Hands-on examples with real text\n",
    "\n",
    "### Why this matters\n",
    "Tokenization is the **first and most critical step** in almost every NLP pipeline. Before you can analyze text, you must break it down into meaningful units. Most downstream tasks (sentiment analysis, NER, machine translation, etc.) depend entirely on quality tokenization.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": [
    "## What is Tokenization?\n",
    "\n",
    "**Tokenization** is the process of splitting raw text into smaller, meaningful units called **tokens**. Tokens are usually words or sentences, but can be characters or subword units depending on your use case.\n",
    "\n",
    "### Example:\n",
    "- Input: `\"Her cat's name is Luna.\"`\n",
    "- Tokens: `['Her', 'cat', \"'s\", 'name', 'is', 'Luna', '.']`\n",
    "\n",
    "### Why tokenize?\n",
    "- Computers process discrete units, not continuous streams\n",
    "- Enables counting word frequencies, building vocabularies, and creating features\n",
    "- Necessary for sentence-level analysis (sentiment per sentence, etc.)\n",
    "\n",
    "**Trade-off**: Simple splitting (e.g., on spaces) doesn't handle punctuation, contractions, or other edge cases well. NLTK's tokenizers use trained models to handle these intelligently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d3ee382-a244-488a-8ba4-a602b4157118",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import NLTK (Natural Language Toolkit) - one of the most popular NLP libraries\n",
    "import nltk\n",
    "\n",
    "# Download 'punkt_tab' - a pre-trained tokenizer model for English\n",
    "# This model learned from millions of sentences how to properly split text\n",
    "# You only need to run this once per environment\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "# Import two key tokenization functions:\n",
    "# - sent_tokenize: splits text into sentences\n",
    "# - word_tokenize: splits text (or sentences) into words\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "data-intro",
   "metadata": {},
   "source": [
    "## Step 1: Prepare Sample Text\n",
    "\n",
    "Let's create a simple example with two sentences. Notice the possessive apostrophes (cat's, dog's) - these will test how well the tokenizer handles edge cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e4fec7-717b-4dc4-8a39-8f6dc6c505f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store a multi-sentence text with possessives as a test case\n",
    "# Possessives ('s) are tricky - the tokenizer must decide whether to attach or separate them\n",
    "sentences = \"Her cat's name is Luna. Her dog's name is Max\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sentence-tokenization",
   "metadata": {},
   "source": [
    "## Step 2: Sentence Tokenization\n",
    "\n",
    "The first level of tokenization is **sentence splitting**. The `sent_tokenize()` function identifies sentence boundaries (typically marked by `.`, `!`, or `?`).\n",
    "\n",
    "### Why sentence tokenization first?\n",
    "- Sentences are often the relevant unit for analysis (e.g., sentiment per sentence)\n",
    "- Many NLP models process one sentence at a time\n",
    "- Helps avoid errors: word from one sentence shouldn't be confused with context from another\n",
    "\n",
    "### How it works:\n",
    "NLTK's `sent_tokenize()` uses a trained neural network that learned patterns of sentence endings. It handles edge cases like abbreviations (e.g., \"Dr. Smith\") better than naive period-splitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ecfd5fa-2eaa-4d84-a0a8-0962e5bfb675",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use sent_tokenize to split the text into sentences\n",
    "# This returns a list of strings, one per sentence\n",
    "sentence_tokens = sent_tokenize(sentences)\n",
    "\n",
    "# Let's inspect what we got\n",
    "print(\"Number of sentences:\", len(sentence_tokens))\n",
    "print(\"\\nSentences:\")\n",
    "for i, sent in enumerate(sentence_tokens, 1):\n",
    "    print(f\"  {i}. {sent}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "word-tokenization",
   "metadata": {},
   "source": [
    "## Step 3: Word Tokenization\n",
    "\n",
    "Now that we have sentences, we split each into individual words. The `word_tokenize()` function is more complex than simple space-splitting because it must:\n",
    "\n",
    "1. **Separate punctuation** from words (e.g., \"Luna.\" ‚Üí \"Luna\", \".\")\n",
    "2. **Handle contractions** intelligently (e.g., \"don't\" ‚Üí \"do\", \"n't\")\n",
    "3. **Respect possessives** (e.g., \"cat's\" ‚Üí \"cat\", \"'s\")\n",
    "\n",
    "### Common beginner mistake:\n",
    "‚ùå **Wrong**: Using `.split()` naively\n",
    "```python\n",
    "\"Luna.\".split()  # Returns ['Luna.'] - punctuation still attached!\n",
    "```\n",
    "\n",
    "‚úÖ **Right**: Using `word_tokenize()`\n",
    "```python\n",
    "word_tokenize(\"Luna.\")  # Returns ['Luna', '.'] - punctuation separated\n",
    "```\n",
    "\n",
    "### Trade-off:\n",
    "NLTK's word tokenizer is quite aggressive about splitting contractions and possessives. Some systems prefer to keep \"don't\" as one token, while others split it to \"do\" + \"n't\". Choose based on your downstream task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0a3c41d-d4d0-4707-bade-1aa9cfbe08c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's tokenize the second sentence into words\n",
    "# sentence_tokens[1] is \"Her dog's name is Max\"\n",
    "second_sentence = sentence_tokens[1]\n",
    "word_tokens = word_tokenize(second_sentence)\n",
    "\n",
    "print(f\"Original sentence: {second_sentence}\")\n",
    "print(f\"\\nWord tokens: {word_tokens}\")\n",
    "print(f\"\\nNumber of tokens: {len(word_tokens)}\")\n",
    "\n",
    "# Notice how 'dog's' was split into ['dog', \"'s\"]\n",
    "# This is correct behavior - the apostrophe is a separate linguistic unit (possessive marker)\n",
    "print(\"\\nToken breakdown:\")\n",
    "for i, token in enumerate(word_tokens):\n",
    "    print(f\"  {i}: '{token}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "advanced-tokenization",
   "metadata": {},
   "source": [
    "## Advanced: Tokenize All Sentences\n",
    "\n",
    "In practice, you'll want to tokenize all sentences. Here's a complete workflow:\n",
    "\n",
    "### Workflow:\n",
    "1. Split text into sentences\n",
    "2. For each sentence, split into words\n",
    "3. Store results in a structured format (e.g., list of lists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "advanced-example",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete two-level tokenization example\n",
    "text = \"Her cat's name is Luna. Her dog's name is Max\"\n",
    "\n",
    "# Step 1: Sentence tokenization\n",
    "sentences = sent_tokenize(text)\n",
    "\n",
    "# Step 2: Word tokenization for each sentence\n",
    "all_word_tokens = []\n",
    "for sent in sentences:\n",
    "    words = word_tokenize(sent)  # Split sentence into words\n",
    "    all_word_tokens.append(words)\n",
    "\n",
    "# Display results\n",
    "print(\"Complete tokenization:\")\n",
    "print(\"=\" * 50)\n",
    "for sent_id, sentence in enumerate(sentences, 1):\n",
    "    print(f\"\\nSentence {sent_id}: {sentence}\")\n",
    "    print(f\"Tokens: {all_word_tokens[sent_id - 1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "key-takeaways",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "1. **Tokenization is essential**: Almost every NLP task starts here\n",
    "2. **Two-level approach**: Sentence ‚Üí Words (not the other way around)\n",
    "3. **Use pre-trained models**: NLTK's tokenizers are better than naive string splitting because they handle edge cases\n",
    "4. **Punctuation matters**: It gets separated from words, which is useful for downstream processing\n",
    "5. **Understand your tokenizer's behavior**: Different systems split contractions/possessives differently\n",
    "\n",
    "## Next steps:\n",
    "- Try tokenizing text with contractions like \"don't\", \"can't\", \"it's\"\n",
    "- Compare results from NLTK with simple `.split()` to see the difference\n",
    "- Move on to **normalization** (lowercasing, removing punctuation) in the next notebook"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_course_env",
   "language": "python",
   "name": "nlp_course_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
