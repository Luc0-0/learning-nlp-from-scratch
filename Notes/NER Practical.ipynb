{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "anchor-section",
   "metadata": {},
   "source": [
    "# ðŸ“Œ Topic: Named Entity Recognition - Practical Applications\n",
    "\n",
    "### What you will learn\n",
    "- How to extract entities from real news articles\n",
    "- Building a news entity database\n",
    "- Analyzing entity distributions in real-world text\n",
    "- Creating structured data from unstructured news\n",
    "- Handling errors and edge cases in production NER\n",
    "- Entity frequency analysis and visualization\n",
    "\n",
    "### Why this matters\n",
    "Real-world NER isn't just about identifying entitiesâ€”it's about extracting actionable insights from text. News articles contain people, organizations, locations, and dates that can be indexed, searched, and analyzed. This notebook shows how NER powers news aggregation systems, knowledge graphs, and content recommendation engines.\n",
    "\n",
    "### Data source\n",
    "BBC News dataset (available in `Data/bbc_news.csv`) containing 2,000+ articles with titles, publication dates, and descriptions.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": [
    "## Real-World Scenario\n",
    "\n",
    "Imagine you're building a **news analytics platform**:\n",
    "- Users want to search news by people mentioned (\"Find articles about Elon Musk\")\n",
    "- Editors need to organize articles by location (\"UK news\", \"Tech news from Silicon Valley\")\n",
    "- Algorithms need to find relationships (\"Which companies did this person work for?\")\n",
    "\n",
    "NER makes this possible by automatically extracting structured data from unstructured text.\n",
    "\n",
    "### What we'll do:\n",
    "1. Load BBC News dataset (1,000+ articles)\n",
    "2. Extract all entities using spaCy NER\n",
    "3. Build an entity database\n",
    "4. Analyze what entities appear most frequently\n",
    "5. Visualize entity distributions\n",
    "6. Handle edge cases and errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2fd5d99-8e1f-4329-a7e6-c8c64b390990",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries for data processing and NER\n",
    "import nltk  # NLTK for tokenization and text processing\n",
    "from nltk.tokenize import word_tokenize  # Split text into words\n",
    "from nltk.corpus import stopwords  # Common words to filter\n",
    "from nltk.stem import WordNetLemmatizer  # Normalize words\n",
    "\n",
    "import spacy  # Modern NLP library with NER\n",
    "import re  # Regular expressions for text cleaning\n",
    "import pandas as pd  # Data manipulation and analysis\n",
    "import matplotlib.pyplot as plt  # Visualization\n",
    "from collections import Counter  # Count entity frequencies\n",
    "\n",
    "print(\"âœ“ All libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "data-loading",\n",
   "metadata": {},\n",
   "source": [
    "## Step 1: Load Real News Data\n",
    "\n",
    "We're using the BBC News dataset from the `Data/` folder. It contains real news articles with titles and descriptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,\n",
   "id": "1cca17ae-8d35-403b-9d96-85f435db7380",\n",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load BBC News dataset\n",
    "# Data source: Data/bbc_news.csv\n",
    "# Contains: title, publication date, link, description for 1,000+ articles\n",
    "bbc_news = pd.read_csv(\"../Data/bbc_news.csv\")\n",
    "\n",
    "print(f\"âœ“ Loaded {len(bbc_news)} news articles\")\n",
    "print(f\"Columns: {list(bbc_news.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,\n",
   "id": "explore-data",\n",
   "metadata": {},\n",
   "outputs": [],
   "source": [
    "# Examine the data structure\n",
    "print(\"First 5 articles:\")\n",
    "print(bbc_news[['title', 'pubDate', 'description']].head())\n",
    "print(f\"\\nDataset shape: {bbc_news.shape}\")\n",
    "print(f\"\\nMissing values:\\n{bbc_news.isnull().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "load-nlp-model",\n",
   "metadata": {},\n",
   "source": [
    "## Step 2: Load spaCy NER Model\n",
    "\n",
    "Initialize the spaCy NER pipeline that will identify entities in news text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,\n",
   "id": "load-spacy",\n",
   "metadata": {},\n",
   "outputs": [],\n",
   "source": [
    "# Load spaCy's NER model\n",
    "# This model was trained on news text, so it performs well on news articles\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "print(\"âœ“ spaCy model loaded\")\n",
    "print(f\"Pipeline components: {nlp.pipe_names}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "extract-entities",\n",
   "metadata": {},\n",
   "source": [
    "## Step 3: Extract Entities from All Articles\n",
    "\n",
    "Process each article and extract all entities. This creates a searchable database of entities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,\n",
   "id": "entity-extraction",\n",
   "metadata": {},\n",
   "outputs": [],\n",
    "source": [
    "# Extract entities from all articles\n",
    "# This may take a minute or two for 1,000+ articles\n",
    "all_entities = []\n",
    "\n",
    "print(\"Extracting entities from articles...\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Process each article\n",
    "for idx, row in bbc_news.iterrows():\n",
    "    # Get article title and description\n",
    "    article_text = f\"{row['title']}. {row['description']}\"\n",
    "    \n",
    "    # Skip if text is missing\n",
    "    if pd.isna(article_text):\n",
    "        continue\n",
    "    \n",
    "    # Process text with spaCy NER\n",
    "    doc = nlp(article_text)\n",
    "    \n",
    "    # Extract entities\n",
    "    for entity in doc.ents:\n",
    "        all_entities.append({\n",
    "            'entity_text': entity.text,\n",
    "            'entity_type': entity.label_,\n",
    "            'article_title': row['title'],\n",
    "            'article_date': row['pubDate']\n",
    "        })\n",
    "    \n",
    "    # Progress indicator\n",
    "    if (idx + 1) % 100 == 0:\n",
    "        print(f\"Processed {idx + 1} articles, found {len(all_entities)} entities\")\n",
    "\n",
    "print(f\"\\nâœ“ Complete! Found {len(all_entities)} total entities\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "entity-database",\n",
   "metadata": {},\n",
   "source": [
    "## Step 4: Create Structured Entity Database\n",
    "\n",
    "Convert the extracted entities into a DataFrame for analysis and searching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,\n",
   "id": "create-entity-db",\n",
   "metadata": {},\n",
   "outputs": [],\n",
    "source": [
    "# Convert to DataFrame for easier analysis\n",
    "entities_df = pd.DataFrame(all_entities)\n",
    "\n",
    "print(f\"Entity Database:\")\n",
    "print(f\"Total entities: {len(entities_df)}\")\n",
    "print(f\"Unique entities: {entities_df['entity_text'].nunique()}\")\n",
    "print(f\"\\nEntity type distribution:\")\n",
    "print(entities_df['entity_type'].value_counts())\n",
    "\n",
    "print(\"\\nSample entities:\")\n",
    "print(entities_df.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "entity-analysis",\n",
   "metadata": {},
   "source": [
    "## Step 5: Analyze Most Common Entities\n",
    "\n",
    "Find the most frequently mentioned entitiesâ€”these tell us what news topics are important."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,\n",
   "id": "top-entities",\n",
   "metadata": {},\n",
   "outputs": [],\n",
    "source": [
    "# Find most common entities overall\n",
    "entity_counts = entities_df['entity_text'].value_counts()\n",
    "\n",
    "print(\"Top 20 Most Mentioned Entities:\")\n",
    "print(\"=\" * 60)\n",
    "for entity, count in entity_counts.head(20).items():\n",
    "    print(f\"{entity:30} â†’ {count:4} mentions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "person-analysis",\n",
   "metadata": {},\n",
   "source": [
    "## Step 6: Analyze by Entity Type\n",
    "\n",
    "Let's break down the most important people, organizations, and locations in the news."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,\n",
   "id": "by-type-analysis",\n",
   "metadata": {},\n",
   "outputs": [],\n",
    "source": [
    "# Top PERSON entities (people mentioned)\n",
    "person_entities = entities_df[entities_df['entity_type'] == 'PERSON']['entity_text'].value_counts()\n",
    "print(\"Top 15 People Mentioned:\")\n",
    "print(\"=\" * 40)\n",
    "for person, count in person_entities.head(15).items():\n",
    "    print(f\"{person:25} â†’ {count:3} articles\")\n",
    "\n",
    "# Top ORG entities (organizations)\n",
    "print(\"\\n\\nTop 15 Organizations Mentioned:\")\n",
    "print(\"=\" * 40)\n",
    "org_entities = entities_df[entities_df['entity_type'] == 'ORG']['entity_text'].value_counts()\n",
    "for org, count in org_entities.head(15).items():\n",
    "    print(f\"{org:25} â†’ {count:3} articles\")\n",
    "\n",
    "# Top GPE entities (locations)\n",
    "print(\"\\n\\nTop 15 Locations Mentioned:\")\n",
    "print(\"=\" * 40)\n",
    "gpe_entities = entities_df[entities_df['entity_type'] == 'GPE']['entity_text'].value_counts()\n",
    "for location, count in gpe_entities.head(15).items():\n",
    "    print(f\"{location:25} â†’ {count:3} articles\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "visualization",\n",
   "metadata": {},\n",
   "source": [
    "## Step 7: Visualize Entity Distributions\n",
    "\n",
    "Bar charts show the most important topics covered in the news."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,\n",
   "id": "visualize-entities",\n",
   "metadata": {},\n",
   "outputs": [],\n",
    "source": [
    "# Visualize entity type distribution\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Entity type distribution\n",
    "entity_type_counts = entities_df['entity_type'].value_counts()\n",
    "axes[0, 0].bar(entity_type_counts.index, entity_type_counts.values, color='steelblue')\n",
    "axes[0, 0].set_title('Entity Types Distribution', fontsize=12, fontweight='bold')\n",
    "axes[0, 0].set_xlabel('Entity Type')\n",
    "axes[0, 0].set_ylabel('Count')\n",
    "axes[0, 0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Top 10 people\n",
    "person_top10 = person_entities.head(10)\n",
    "axes[0, 1].barh(person_top10.index, person_top10.values, color='coral')\n",
    "axes[0, 1].set_title('Top 10 People Mentioned', fontsize=12, fontweight='bold')\n",
    "axes[0, 1].set_xlabel('Mentions')\n",
    "\n",
    "# Top 10 organizations\n",
    "org_top10 = org_entities.head(10)\n",
    "axes[1, 0].barh(org_top10.index, org_top10.values, color='lightgreen')\n",
    "axes[1, 0].set_title('Top 10 Organizations', fontsize=12, fontweight='bold')\n",
    "axes[1, 0].set_xlabel('Mentions')\n",
    "\n",
    "# Top 10 locations\n",
    "gpe_top10 = gpe_entities.head(10)\n",
    "axes[1, 1].barh(gpe_top10.index, gpe_top10.values, color='lightyellow')\n",
    "axes[1, 1].set_title('Top 10 Locations', fontsize=12, fontweight='bold')\n",
    "axes[1, 1].set_xlabel('Mentions')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"âœ“ Visualizations complete\")"
   ]
  },
  {
   "cell_type": "markdown",\n",
   "id": "search-functionality",\n",
   "metadata": {},\n",
   "source": [
    "## Step 8: Build a Search Function\n",
    "\n",
    "Create a practical search tool to find articles mentioning specific entities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,\n",
   "id": "search-function",\n",
   "metadata": {},\n",
   "outputs": [],\n",
    "source": [
    "def search_by_entity(entity_name, entity_type=None):\n",
    "    \"\"\"\n",
    "    Search articles mentioning a specific entity.\n",
    "    \n",
    "    Args:\n",
    "        entity_name: Name of the entity to search for\n",
    "        entity_type: Optional - filter by entity type (PERSON, ORG, GPE)\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame of articles mentioning the entity\n",
    "    \"\"\"\n",
    "    # Case-insensitive search\n",
    "    matches = entities_df[entities_df['entity_text'].str.contains(entity_name, case=False, na=False)]\n",
    "    \n",
    "    # Optional: filter by entity type\n",
    "    if entity_type:\n",
    "        matches = matches[matches['entity_type'] == entity_type]\n",
    "    \n",
    "    # Get unique articles\n",
    "    articles = matches[['article_title', 'article_date', 'entity_text', 'entity_type']].drop_duplicates()\n",
    "    \n",
    "    return articles\n",
    "\n",
    "# Example searches\n",
    "print(\"Search Example 1: Find all articles mentioning 'UK'\")\n",
    "print(\"=\" * 60)\n",
    "uk_articles = search_by_entity('UK')\n",
    "print(f\"Found {len(uk_articles)} articles\")\n",
    "print(uk_articles.head(5))\n",
    "\n",
    "print(\"\\n\\nSearch Example 2: Find all articles mentioning people named 'Boris'\")\n",
    "print(\"=\" * 60)\n",
    "boris_articles = search_by_entity('Boris', entity_type='PERSON')\n",
    "print(f\"Found {len(boris_articles)} articles\")\n",
    "print(boris_articles.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "handling-errors",\n",
   "metadata": {},\n",
   "source": [
    "## Step 9: Understanding NER Errors in Real Data\n",
    "\n",
    "Real-world text introduces errors that don't appear in clean examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,\n",
   "id": "error-analysis",\n",
   "metadata": {},\n",
   "outputs": [],\n",
    "source": [
    "# Common NER errors found in real news\n",
    "print(\"Common NER Challenges in Real News:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Error 1: Inconsistent capitalization\n",
    "print(\"\\n1. CAPITALIZATION INCONSISTENCY:\")\n",
    "print(\"   'UK' vs 'uk' vs 'U.K.' all count as different entities\")\n",
    "inconsistent = entities_df[entities_df['entity_text'].str.lower().str.contains('uk')]\n",
    "print(f\"   Found {len(inconsistent.entity_text.unique())} variations of UK\")\n",
    "print(f\"   Examples: {list(inconsistent['entity_text'].unique()[:5])}\")\n",
    "\n",
    "# Error 2: Partial entity matches\n",
    "print(\"\\n2. PARTIAL ENTITY MATCHES:\")\n",
    "print(\"   'Harry' vs 'Harry Potter' may be tagged separately\")\n",
    "harry = entities_df[entities_df['entity_text'].str.contains('Harry', case=False)]\n",
    "print(f\"   Found {len(harry['entity_text'].unique())} Harry variations\")\n",
    "print(f\"   Examples: {list(harry['entity_text'].unique()[:5])}\")\n",
    "\n",
    "# Error 3: Common false positives\n",
    "print(\"\\n3. POTENTIAL FALSE POSITIVES:\")\n",
    "print(\"   Some entities might be wrongly classified\")\n",
    "# Check for single letters or very short entities (usually errors)\n",
    "short_entities = entities_df[entities_df['entity_text'].str.len() <= 2]\n",
    "print(f\"   Found {len(short_entities)} entities with 2 or fewer characters\")\n",
    "print(f\"   These may be errors: {short_entities['entity_text'].value_counts().head(5)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "entity-normalization",\n",
   "metadata": {},\n",
   "source": [
    "## Step 10: Entity Normalization (Fixing Common Errors)\n",
    "\n",
    "Clean up entity variations to create a unified database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,\n",
   "id": "normalize-entities",\n",
   "metadata": {},\n",
   "outputs": [],\n",
    "source": [
    "# Function to normalize entities\n",
    "def normalize_entity(entity_text):\n",
    "    \"\"\"\n",
    "    Normalize entity text to handle common variations.\n",
    "    Examples: 'U.K.' â†’ 'UK', 'United States' â†’ 'US'\n",
    "    \"\"\"\n",
    "    # Remove extra whitespace\n",
    "    entity = entity_text.strip()\n",
    "    \n",
    "    # Common abbreviation normalizations\n",
    "    replacements = {\n",
    "        'U.K.': 'UK',\n",
    "        'U.S.': 'US',\n",
    "        'U.S.A.': 'USA',\n",
    "        'U.S.S.R': 'USSR',\n",
    "    }\n",
    "    \n",
    "    for old, new in replacements.items():\n",
    "        entity = entity.replace(old, new)\n",
    "    \n",
    "    return entity\n",
    "\n",
    "# Apply normalization\n",
    "entities_df['entity_normalized'] = entities_df['entity_text'].apply(normalize_entity)\n",
    "\n",
    "# Check normalization results\n",
    "print(\"Normalization Results:\")\n",
    "print(\"=\" * 60)\n",
    "normalized_counts = entities_df['entity_normalized'].value_counts()\n",
    "print(f\"\\nBefore normalization: {len(entities_df['entity_text'].unique())} unique entities\")\n",
    "print(f\"After normalization: {len(normalized_counts)} unique entities\")\n",
    "print(f\"\\nTop 20 normalized entities:\")\n",
    "print(normalized_counts.head(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "key-takeaways",\n",
   "metadata": {},\n",
   "source": [
    "## Key Takeaways\n",
    "\n",
    "1. **Scale matters**: NER on 1,000+ articles reveals patterns not visible in single articles\n",
    "2. **Real data is messy**: Capitalization, abbreviations, and variations require normalization\n",
    "3. **Entity frequency analysis reveals topics**: What entities appear most = what news is about\n",
    "4. **Search functionality**: Extracting entities makes articles searchable and linkable\n",
    "5. **Error handling is crucial**: Short entities, inconsistent capitalization, false positives are real\n",
    "6. **Normalization improves quality**: Unifying entity variations makes the database more useful\n",
    "7. **Visualization helps understanding**: Charts show what's important in the news\n",
    "\n",
    "## Next Steps:\n",
    "- **Entity Linking**: Connect entities to Wikipedia or knowledge bases\n",
    "- **Relation Extraction**: Find relationships between entities (\"Company X was founded by Person Y\")\n",
    "- **Temporal Analysis**: Track how entity frequencies change over time\n",
    "- **Knowledge Graph**: Build a graph of entities and relationships\n",
    "- **Custom NER**: Train models for domain-specific entities (stock tickers, medical terms)"
   ]
  },\n",
   "cell_type": "markdown",\n",
   "id": "practice",\n",
   "metadata": {},\n",
   "source": [
    "## Practice Exercises\n",
    "\n",
    "Try these on your own:"
   ]
  },\n",
   "cell_type": "code",\n",
   "execution_count": null,\n",
   "id": "practice-1",\n",
   "metadata": {},\n",
   "outputs": [],\n",
    "source": [
    "# Exercise 1: Find articles mentioning a specific company\n",
    "# Modify and search for your company of interest\n",
    "company = 'Apple'  # Change this to search for different companies\n",
    "company_articles = search_by_entity(company, entity_type='ORG')\n",
    "print(f\"Articles mentioning {company}:\")\n",
    "print(company_articles)"
   ]
  },\n",
   "cell_type": "code",\n",
   "execution_count": null,\n",
   "id": "practice-2",\n",
   "metadata": {},\n",
   "outputs": [],\n",
    "source": [
    "# Exercise 2: Find articles about a specific country/region\n",
    "location = 'China'  # Change to search different locations\n",
    "location_articles = search_by_entity(location, entity_type='GPE')\n",
    "print(f\"\\nArticles about {location}:\")\n",
    "print(f\"Total: {len(location_articles)} articles\")\n",
    "print(location_articles.head(10))"
   ]
  },\n",
   "cell_type": "code",\n",
   "execution_count": null,\n",
   "id": "practice-3",\n",
   "metadata": {},\n",
   "outputs": [],\n",
    "source": [
    "# Exercise 3: Create a co-occurrence analysis\n",
    "# Find entities that appear together in articles\n",
    "# Example: People and companies mentioned in the same article\n",
    "\n",
    "def find_entity_pairs(article_title, entity_type1='PERSON', entity_type2='ORG'):\n",
    "    \"\"\"\n",
    "    Find pairs of entities that appear in the same article.\n",
    "    \"\"\"\n",
    "    article_entities = entities_df[entities_df['article_title'] == article_title]\n",
    "    \n",
    "    type1_entities = article_entities[article_entities['entity_type'] == entity_type1]['entity_text'].unique()\n",
    "    type2_entities = article_entities[article_entities['entity_type'] == entity_type2]['entity_text'].unique()\n",
    "    \n",
    "    return type1_entities, type2_entities\n",
    "\n",
    "# Test on first article\n",
    "first_article = bbc_news.iloc[0]['title']\n",
    "people, orgs = find_entity_pairs(first_article)\n",
    "print(f\"Article: {first_article}\")\n",
    "print(f\"People mentioned: {list(people)}\")\n",
    "print(f\"Organizations mentioned: {list(orgs)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_course_env",
   "language": "python",
   "name": "nlp_course_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
