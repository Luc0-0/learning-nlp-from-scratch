{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "anchor-section",
   "metadata": {},
   "source": [
    "# Hugging Face Transformers\n",
    "\n",
    "## Learning Objectives\n",
    "By the end of this notebook, you'll understand:\n",
    "1. **Pre-trained models and pipelines** - how to use transformer models without training\n",
    "2. **Task-specific applications** - sentiment analysis, NER, zero-shot classification\n",
    "3. **Tokenization** - converting text to numerical representations for models\n",
    "4. **Model inference** - using raw models with PyTorch/TensorFlow\n",
    "5. **Model persistence** - saving and loading pre-trained models\n",
    "\n",
    "## Why This Matters\n",
    "Hugging Face Transformers democratizes access to state-of-the-art NLP models. Instead of:\n",
    "- Training models from scratch (months of GPU time, massive datasets)\n",
    "- Implementing transformer architecture yourself\n",
    "- Managing complex dependencies\n",
    "\n",
    "You can now:\n",
    "- Load production-ready models in 2 lines of code\n",
    "- Fine-tune on your data (hours instead of months)\n",
    "- Focus on problem-solving instead of implementation\n",
    "\n",
    "## Real-World Context\n",
    "Transformers power modern NLP:\n",
    "- BERT, GPT, BART: Foundation models for language understanding\n",
    "- Specialized models: Domain-specific, multilingual, efficient versions\n",
    "- Pipeline abstraction: Hide complexity, expose functionality\n",
    "- Production deployment: Save/load models, batch processing, optimization\n",
    "\n",
    "This notebook covers the practical workflow: load → tokenize → predict → save."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pipelines-intro",
   "metadata": {},
   "source": [
    "## Phase 1: High-Level Pipelines\n",
    "\n",
    "The `pipeline()` function abstracts away complexity:\n",
    "- Automatically downloads the model\n",
    "- Handles tokenization internally\n",
    "- Returns human-readable results\n",
    "\n",
    "This is the simplest way to use transformers for quick prototyping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5dd977b4-26d2-4d97-9987-a564d06218f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the high-level pipeline API for easy model usage\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9ba7c8fd-bdb3-454a-9c9e-104554423b7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "# Create a sentiment analysis pipeline\n",
    "# Automatically downloads the distilbert model fine-tuned on SST-2 dataset\n",
    "# distilbert is a smaller, faster version of BERT (40% smaller, 60% faster)\n",
    "sentiment_classifier = pipeline(\"sentiment-analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8300d1ed-02d8-4f8d-a77f-cdb9e5f091c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample text for sentiment analysis\n",
    "# This demonstrates sentiment detection on an optimistic statement\n",
    "text = \"The technological advances in artificial intelligence are remarkable and promising for solving complex problems worldwide\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "68ca4c4f-66c0-4865-a81f-b8aab08699e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9998711347579956}]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run sentiment analysis on the text\n",
    "# Returns label (POSITIVE/NEGATIVE) and confidence score\n",
    "sentiment_classifier(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d7d19621-8e16-4eb8-a712-ada9d2cca121",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dslim/bert-base-NER were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "# Create Named Entity Recognition pipeline\n",
    "# Uses BERT fine-tuned on NER task\n",
    "# aggregation_strategy=\"max\": if subword tokens are split, use max probability\n",
    "# Other options: \"simple\" (no aggregation), \"first\", \"average\"\n",
    "ner = pipeline(\n",
    "    \"ner\",\n",
    "    model=\"dslim/bert-base-NER\",\n",
    "    aggregation_strategy=\"max\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3267104b-2336-4af8-b8a1-f5ff7809c94e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run NER on the same text\n",
    "# Extracts entities: PERSON, ORG, LOC, etc.\n",
    "# Returns: entity_group (type), word, score, start/end positions\n",
    "ner(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3d529d79-344f-4bff-b4e1-aaf2bdece493",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "# Zero-shot classification: classify without training data for specific categories\n",
    "# Uses BART model trained on Natural Language Inference (NLI) task\n",
    "# Can classify any text into arbitrary categories without fine-tuning\n",
    "zeroshot_classifier = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7c501f93-fc39-4b31-8e00-efbb652ce51e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text to classify with zero-shot approach\n",
    "sequence_to_classify = \"Exploring new cultures and visiting landmark sites across continents\"\n",
    "# Define possible categories - can be anything!\n",
    "candidate_label = ['travel', 'food', 'sports']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "06a9ba61-8ce7-4f19-b25c-801b4e0827d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sequence': 'Exploring new cultures and visiting landmark sites across continents',\n",
       " 'labels': ['travel', 'sports', 'food'],\n",
       " 'scores': [0.9943504333496094, 0.0029930926393717527, 0.0026564467698335648]}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run zero-shot classification\n",
    "# Returns ranked labels with confidence scores\n",
    "zeroshot_classifier(sequence_to_classify, candidate_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ba656f0-67a6-4fe3-a1a5-951296aecd5e",
   "metadata": {},
   "source": [
    "## Phase 2: Tokenization Deep Dive\n",
    "\n",
    "Pipelines hide tokenization. Understanding tokenizers is crucial:\n",
    "- Different models use different tokenization strategies\n",
    "- Tokenization affects model input/output\n",
    "- Subword tokenization (BPE, WordPiece) handles rare words\n",
    "- Special tokens ([CLS], [SEP], [PAD]) serve specific purposes\n",
    "\n",
    "### Pre-trained Tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c2b9ebf9-3da1-4ea4-88fc-01707109d8d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import AutoTokenizer for automatic model-appropriate tokenizer loading\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "bafcd4d2-e4be-4b6f-b7a5-11ce5eefe28b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify which model's tokenizer to load\n",
    "# bert-base-uncased: Lowercase English BERT, widely used baseline\n",
    "model = \"bert-base-uncased\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6e37da56-7e92-472e-ad11-01516ab8f8de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre-trained tokenizer for BERT\n",
    "# Automatically downloads vocabulary and tokenization rules\n",
    "tokenizer = AutoTokenizer.from_pretrained(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8c81e2aa-7936-4698-ba9b-fb2f01d6b109",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [101, 1996, 10660, 9849, 1999, 7976, 4454, 2024, 9487, 1998, 10015, 2005, 13729, 3375, 3471, 4969, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "# Tokenize the text - converts text to input IDs and attention masks\n",
    "# Returns dict with: input_ids (token indices), token_type_ids (sentence boundaries), attention_mask (padding mask)\n",
    "input_ids = tokenizer(text)\n",
    "print(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "cfdc1415-39f5-44ba-a22a-5095af169308",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'technological', 'advances', 'in', 'artificial', 'intelligence', 'are', 'remarkable', 'and', 'promising', 'for', 'solving', 'complex', 'problems', 'worldwide']\n"
     ]
    }
   ],
   "source": [
    "# Get token strings (not IDs)\n",
    "# Notice: words are split into subwords (## prefix indicates continuation)\n",
    "# Example: \"artificial\" might become [\"artificial\"] or [\"arti\", \"##ficial\"]\n",
    "tokens = tokenizer.tokenize(text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a4d988e3-2db7-4b7e-8ddd-b2c4006ac13d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1996, 10660, 9849, 1999, 7976, 4454, 2024, 9487, 1998, 10015, 2005, 13729, 3375, 3471, 4969]\n"
     ]
    }
   ],
   "source": [
    "token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "print(token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f1a9145f-78b8-4cca-a869-67a0b45c7521",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the technological advances in artificial intelligence are remarkable and promising for solving complex problems worldwide\n"
     ]
    }
   ],
   "source": [
    "decoded_ids = tokenizer.decode(token_ids)\n",
    "print(decoded_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "5eb1dfec-d61c-493c-b4be-3d0faef007e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 =\"xlnet-base-cased\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "23f28aea-5fec-4a3c-9d81-72a66075a2b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer2 =AutoTokenizer.from_pretrained(model2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "cb3b5c84-7c07-448c-bf94-126eb027192c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [32, 8647, 8809, 25, 8298, 2503, 41, 7459, 21, 7559, 28, 12901, 1881, 708, 2805, 4, 3], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "input_ids2 = tokenizer2(text)\n",
    "print(input_ids2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "963f1d55-8a4d-4f59-924c-4758dd8f7e22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The technological advances in artificial intelligence are remarkable and promising for solving complex problems worldwide<sep><cls>\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer2.decode(input_ids2[\"input_ids\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c9ecaa7e-1e03-4794-b06a-e5d202da47a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁The', '▁technological', '▁advances', '▁in', '▁artificial', '▁intelligence', '▁are', '▁remarkable', '▁and', '▁promising', '▁for', '▁solving', '▁complex', '▁problems', '▁worldwide']\n"
     ]
    }
   ],
   "source": [
    "tokens2 = tokenizer2.tokenize(text)\n",
    "print(tokens2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "d7b16d4e-a911-4c2a-9578-775f96f94edd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[32, 8647, 8809, 25, 8298, 2503, 41, 7459, 21, 7559, 28, 12901, 1881, 708, 2805]\n"
     ]
    }
   ],
   "source": [
    "ids2=tokenizer2.convert_tokens_to_ids(tokens2)\n",
    "print(ids2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d89cf04c-24c5-4276-89cc-ba0f9ee86b92",
   "metadata": {},
   "source": [
    "## Special Tokens\n",
    "\n",
    "Special tokens are fundamental to transformer functionality. Each serves a specific purpose:\n",
    "\n",
    "| Token | Purpose | Example |\n",
    "|-------|---------|----------|\n",
    "| [CLS] | Classification token - start of sequence | Added before sentence for sequence-level tasks |\n",
    "| [SEP] | Separation token - marks sentence boundaries | Separates two sentences in sentence-pair tasks |\n",
    "| [PAD] | Padding token - fills sequences to fixed length | Makes all sequences same length for batching |\n",
    "| [UNK] | Unknown token - for out-of-vocabulary words | When word not in model vocabulary |\n",
    "| [MASK] | Mask token - for masked language modeling | Used in pre-training and fill-mask tasks |\n",
    "| [BOS]/[EOS] | Begin/End of sequence - for generation | Marks start/end in sequence-to-sequence models |\n",
    "\n",
    "### Why Special Tokens Matter\n",
    "- **[CLS]** output is used for classification tasks (sentiment, entailment, etc.)\n",
    "- **[SEP]** helps model understand sentence boundaries and relationships\n",
    "- **[PAD]** enables efficient batch processing with variable-length sequences\n",
    "- **[MASK]** is essential for BERT pre-training and fine-tuning objectives\n",
    "\n",
    "Different models use different special tokens - always check model documentation!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fe304a5-3f47-49c6-9f36-2964d886bf6f",
   "metadata": {},
   "source": [
    "## Phase 3: Low-Level API - PyTorch Integration\n",
    "\n",
    "Pipelines are convenient, but sometimes you need fine-grained control:\n",
    "- Custom preprocessing or postprocessing\n",
    "- Batch processing optimization\n",
    "- Access to intermediate representations\n",
    "- Fine-tuning on custom data\n",
    "\n",
    "This phase shows how to use AutoTokenizer + AutoModel directly with PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "80e9106e-65af-457c-8bc2-6f74a9a59e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import AutoModel and AutoTokenizer for direct model access\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch  # PyTorch for tensor operations and GPU support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "a8c8dcfd-02e8-4cd6-8e0f-a6b84e411afc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The technological advances in artificial intelligence are remarkable and promising for solving complex problems worldwide\n"
     ]
    }
   ],
   "source": [
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "e84b3ee0-51ff-4f02-adb9-e2c459ccc279",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [32, 8647, 8809, 25, 8298, 2503, 41, 7459, 21, 7559, 28, 12901, 1881, 708, 2805, 4, 3], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "print(input_ids2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "18af8306-f969-4067-b761-c4ba238355e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer for the sentiment classification model\n",
    "# This tokenizer matches the model's training\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "a7d2d61a-80aa-489e-87e5-7b8e2d52ccf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[  101,  1996, 10660,  9849,  1999,  7976,  4454,  2024,  9487,  1998,\n",
      "         10015,  2005, 13729,  3375,  3471,  4969,   102]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n"
     ]
    }
   ],
   "source": [
    "# Tokenize and return PyTorch tensors (not lists)\n",
    "# return_tensors='pt' produces torch.Tensor instead of lists\n",
    "# Automatically pads to max sequence length\n",
    "input_ids_pt = tokenizer(text, return_tensors='pt')\n",
    "print(input_ids_pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "7210640e-8618-4c25-bb2b-bdc31b1f8e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the fine-tuned model for sequence classification\n",
    "# AutoModelForSequenceClassification automatically selects appropriate model class\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "be471ca9-0b60-4a35-9ef9-a8637771b057",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'POSITIVE'"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run inference: get predictions without computing gradients (faster, lower memory)\n",
    "with torch.no_grad():\n",
    "    # model() returns ModelOutput with logits, hidden_states, etc.\n",
    "    # ** unpacks the dict to pass tokens to model\n",
    "    logits = model(**input_ids_pt).logits\n",
    "\n",
    "# Get the class with highest score\n",
    "predicted_class_id = logits.argmax().item()\n",
    "# Convert ID to label using model's label mapping\n",
    "model.config.id2label[predicted_class_id]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "model-persistence",
   "metadata": {},
   "source": [
    "## Model Persistence and Deployment\n",
    "\n",
    "### Saving Models and Tokenizers\n",
    "After training or fine-tuning, save your model for reproducibility and deployment:\n",
    "\n",
    "```python\n",
    "# Save tokenizer\n",
    "model_directory = \"my_saved_models\"\n",
    "tokenizer.save_pretrained(model_directory)\n",
    "# Creates: vocab.txt, tokenizer_config.json, special_tokens_map.json, tokenizer.json\n",
    "\n",
    "# Save model\n",
    "model.save_pretrained(model_directory)\n",
    "# Creates: pytorch_model.bin (or model.safetensors), config.json\n",
    "```\n",
    "\n",
    "### Loading Saved Models\n",
    "Restore models from disk:\n",
    "\n",
    "```python\n",
    "# Load tokenizer from disk\n",
    "my_tokenizer = AutoTokenizer.from_pretrained(model_directory)\n",
    "\n",
    "# Load model from disk\n",
    "my_model = AutoModelForSequenceClassification.from_pretrained(model_directory)\n",
    "```\n",
    "\n",
    "Works exactly the same as loading from Hugging Face Hub - both point to local or remote model files.\n",
    "\n",
    "### Production Deployment Best Practices\n",
    "- Save model config alongside weights for reproducibility\n",
    "- Version your saved models (model_v1, model_v2, etc.)\n",
    "- Use model.safetensors format (faster loading, more secure)\n",
    "- Implement model versioning in your application\n",
    "- Consider quantization or distillation for deployment efficiency"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "key-takeaways",
   "metadata": {},
   "source": [
    "## Key Takeaways: Hugging Face Transformers\n",
    "\n",
    "### 1. Three Abstraction Levels\n",
    "- **Pipelines**: Simplest, best for quick prototyping (sentiment, NER, Q&A)\n",
    "- **AutoTokenizer + AutoModel**: More control, good for custom workflows\n",
    "- **Raw PyTorch/TensorFlow**: Maximum control, for research and advanced use cases\n",
    "\n",
    "### 2. Tokenization is Critical\n",
    "- Different models use different tokenizers\n",
    "- Subword tokenization (WordPiece, BPE) handles rare words\n",
    "- Special tokens serve specific purposes: [CLS], [SEP], [PAD], [MASK]\n",
    "- Always use the matching tokenizer for your model\n",
    "\n",
    "### 3. Pre-trained > Training from Scratch\n",
    "- Transfer learning: pre-training on massive corpora, fine-tune on your data\n",
    "- Hours of training instead of months\n",
    "- Better results with less data\n",
    "- Democratized: anyone can use SOTA models\n",
    "\n",
    "### 4. Production Considerations\n",
    "- Save models locally for reproducibility\n",
    "- Use return_tensors for batch processing\n",
    "- Optimize models (distillation, quantization) for deployment\n",
    "- Monitor inference latency and memory usage\n",
    "\n",
    "### 5. Common Pitfalls\n",
    "- Mismatch between tokenizer and model\n",
    "- Forgetting torch.no_grad() for inference\n",
    "- Not handling variable-length sequences properly\n",
    "- Using wrong model class for task (e.g., AutoModel vs AutoModelForSequenceClassification)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "practice-exercises",
   "metadata": {},
   "source": [
    "## Practice Exercises\n",
    "\n",
    "### Exercise 1: Compare Different Tokenizers\n",
    "**Objective**: Understand how different tokenizers handle the same text\n",
    "\n",
    "```python\n",
    "sample_text = \"Artificial intelligence is revolutionizing technology worldwide\"\n",
    "tokenizers = {\n",
    "    'bert-base': AutoTokenizer.from_pretrained(\"bert-base-uncased\"),\n",
    "    'roberta': AutoTokenizer.from_pretrained(\"roberta-base\"),\n",
    "    'gpt2': AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "}\n",
    "for name, tokenizer in tokenizers.items():\n",
    "    tokens = tokenizer.tokenize(sample_text)\n",
    "    print(f\"{name}: {len(tokens)} tokens\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Exercise 2: Build a Sentiment Pipeline\n",
    "**Objective**: Classify multiple texts and interpret results\n",
    "\n",
    "```python\n",
    "texts = [\n",
    "    \"This product is amazing and exceeded expectations\",\n",
    "    \"Terrible experience, would not recommend\",\n",
    "    \"It works okay, nothing special\"\n",
    "]\n",
    "classifier = pipeline(\"sentiment-analysis\")\n",
    "for text in texts:\n",
    "    result = classifier(text)[0]\n",
    "    print(f\"{text}: {result['label']} ({result['score']:.3f})\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Exercise 3: Extract Entities from Text\n",
    "**Objective**: Use NER on diverse text types\n",
    "\n",
    "```python\n",
    "texts = [\n",
    "    \"Apple Inc. was founded by Steve Jobs in California\",\n",
    "    \"Barack Obama was elected president in 2008\",\n",
    "    \"The Eiffel Tower is located in Paris, France\"\n",
    "]\n",
    "ner = pipeline(\"ner\")\n",
    "for text in texts:\n",
    "    entities = ner(text)\n",
    "    print(f\"Text: {text}\")\n",
    "    for entity in entities:\n",
    "        print(f\"  {entity['word']}: {entity['entity_group']}\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "1. **Fine-tuning**: Adapt models to domain-specific tasks\n",
    "2. **Model Optimization**: Distillation, quantization for deployment\n",
    "3. **Multi-GPU Training**: Scale training to massive datasets\n",
    "4. **Advanced Architectures**: T5, GPT-2, BART for generation tasks\n",
    "5. **Production Deployment**: FastAPI, ONNX, TorchServe for serving models\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llms_course_env",
   "language": "python",
   "name": "llms_course_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
