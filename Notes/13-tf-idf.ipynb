{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "anchor-section",
            "metadata": {},
            "source": [
                "# ðŸ“Œ Topic: TF-IDF Vectorization\n",
                "\n",
                "### What you will learn\n",
                "- What TF-IDF is and how it improves upon the Bag of Words model\n",
                "- Mathematical concept behind Term Frequency (TF) and Inverse Document Frequency (IDF)\n",
                "- Practical implementation using Scikit-learn's `TfidfVectorizer`\n",
                "- How to interpret the importance of words in a corpus\n",
                "\n",
                "### Why this matters\n",
                "In simple frequency counts (BoW), common words like \"the\", \"is\", and \"and\" often dominate the feature set but carry very little informative value. **TF-IDF** (Term Frequency-Inverse Document Frequency) solves this by penalizing common words and boosting words that are unique to specific documents. It is a cornerstone of search engines and information retrieval systems.\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "intro",
            "metadata": {},
            "source": [
                "## The Math of Importance\n",
                "\n",
                "TF-IDF is the product of two statistics:\n",
                "\n",
                "1.  **Term Frequency (TF)**: How often a word appear in a single document. (Reward frequent words in a document)\n",
                "2.  **Inverse Document Frequency (IDF)**: How common a word is across the *entire* collection of documents. (Penalize words that appear everywhere)\n",
                "\n",
                "**Formula**: `TF-IDF = TF(word, document) * IDF(word, corpus)`\n",
                "\n",
                "A high score means the word is frequent in the current document but rare in others, making it a strong \"signature\" word for that document."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "f7519e50-22d6-4403-97c3-a7a782f14b69",
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd \n",
                "from sklearn.feature_extraction.text import TfidfVectorizer\n",
                "\n",
                "# Sample toy corpus to illustrate weighting\n",
                "corpus = [\n",
                "    \"Natural language processing is a subfield of linguistics and AI.\",\n",
                "    \"Data science combines statistics, data analysis, and machine learning.\",\n",
                "    \"Machine learning implies that the computer learns from data.\",\n",
                "    \"TF-IDF stands for Term Frequency-Inverse Document Frequency.\",\n",
                "    \"Text vectorization converts text into numerical vectors for AI models.\",\n",
                "    \"Python is excellent for data science and natural language processing.\"\n",
                "]"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "vectorization-step",
            "metadata": {},
            "source": [
                "## Step 1: Initialize the TfidfVectorizer\n",
                "\n",
                "Similarly to `CountVectorizer`, the `TfidfVectorizer` handles tokenization and calculation automatically."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "06d1ec27-d4fe-419d-a145-d2362787a85e",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Initialize the vectorizer\n",
                "tfidf_vec = TfidfVectorizer()\n",
                "\n",
                "# Fit and learn the importance weights\n",
                "tfidf_fit = tfidf_vec.fit_transform(corpus)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "display-results",
            "metadata": {},
            "source": [
                "## Step 2: Visualization\n",
                "\n",
                "Let's look at the scores. Note how words that appear in many sentences (like \"and\", \"data\", \"learning\") get lower weights compared to unique words like \"linguistics\" or \"python\"."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "ce3e1e5b-7182-45ce-86f6-bf32579ed597",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Convert the scores into a DataFrame for readability\n",
                "tfidf_df = pd.DataFrame(tfidf_fit.toarray(), columns=tfidf_vec.get_feature_names_out())\n",
                "\n",
                "# Display the matrix\n",
                "tfidf_df"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "key-takeaways",
            "metadata": {},
            "source": [
                "## Key Takeaways\n",
                "\n",
                "1.  **Contextual Importance**: Unlike Bag of Words, TF-IDF understands that some words are naturally more informative than others.\n",
                "2.  **Automatic Noise Reduction**: It naturally down-weights common \"noise\" words without always needing a hard-coded stopword list.\n",
                "3.  **Vector Comparison**: Documents with similar TF-IDF vectors are likely about similar subjects.\n",
                "\n",
                "## Next steps:\n",
                "- Use these TF-IDF vectors as input for **Text Classification** models (Logistic Regression, SVM).\n",
                "- Explore **N-grams** with TF-IDF to capture multi-word phrases (like \"machine learning\" or \"data science\")."
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "nlp_course_env",
            "language": "python",
            "name": "nlp_course_env"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.11.14"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}