{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "anchor-section",
            "metadata": {},
            "source": [
                "# ðŸ“Œ Topic: Latent Semantic Analysis (LSA)\n",
                "\n",
                "### What you will learn\n",
                "- What LSA is and why we use it to find \"hidden\" meanings\n",
                "- Reducing dimensionality of text data to filter out noise\n",
                "- Practical implementation using Gensim's `LsiModel` (Latent Semantic Indexing)\n",
                "- How to group documents based on underlying concepts\n",
                "\n",
                "### Why this matters\n",
                "Synonyms are a big problem in NLP. A search for \"cell phone\" might miss a document about \"smartphones\" because the words are different. **Latent Semantic Analysis (LSA)** solves this by looking at patterns of word co-occurrence. It identifies concepts (topics) rather than just individual words, allowing it to find relationships between documents that don't share exact keywords.\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "intro",
            "metadata": {},
            "source": [
                "## How does LSA work?\n",
                "\n",
                "LSA uses a mathematical technique called **Singular Value Decomposition (SVD)**. It builds a term-document matrix and then compresses it into a much lower-dimensional space. \n",
                "\n",
                "Imagine your dataset has 10,000 unique words. LSA might compress that representation down to just 100 \"topics.\" This compression forces the model to ignore noise and focus on the most important relationships between words."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "296b507c-8602-4fd3-8f45-85d35d3c362c",
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "from gensim import corpora\n",
                "from gensim.models import LsiModel\n",
                "from nltk.tokenize import word_tokenize\n",
                "import nltk\n",
                "\n",
                "# Sample corpus on different themes\n",
                "raw_docs = [\n",
                "    \"The cat and dog were playing in the garden.\",\n",
                "    \"The kitten and puppy were having fun outdoors.\",\n",
                "    \"I love working with natural language processing and Python.\",\n",
                "    \"Data science and machine learning are exciting fields.\",\n",
                "    \"NLP models require large amounts of training text.\"\n",
                "]\n",
                "\n",
                "# Basic preprocessing\n",
                "tokenized_docs = [word_tokenize(doc.lower()) for doc in raw_docs]\n",
                "\n",
                "# Create a dictionary and corpus (bag-of-words) for Gensim\n",
                "dictionary = corpora.Dictionary(tokenized_docs)\n",
                "corpus = [dictionary.doc2bow(text) for text in tokenized_docs]"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "training-step",
            "metadata": {},
            "source": [
                "## Step 1: Building the LSA Model\n",
                "\n",
                "We use the `LsiModel` to reduce our corpus dimensionality. We'll ask it to find 2 main \"topics\" or concepts."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "lsi-training",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Initialize LSA model with 2 topics\n",
                "lsi = LsiModel(corpus, id2word=dictionary, num_topics=2)\n",
                "\n",
                "# Display the topics discovered\n",
                "for idx, topic in lsi.print_topics(-1):\n",
                "    print(f\"Topic {idx}: {topic}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "interpretation",
            "metadata": {},
            "source": [
                "## Step 2: Interpreting the Results\n",
                "\n",
                "Notice how the model puts words related to animals in one topic and words related to tech/NLP in another. The negative vs positive values show how much certain words pull a document toward or away from that specific concept."
            ]
        },
        {
            "cell_type": "markdown",
            "id": "key-takeaways",
            "metadata": {},
            "source": [
                "## Key Takeaways\n",
                "\n",
                "1.  **Noise Reduction**: By lowering dimensions, we throw away secondary word variations and keep the core semantic structure.\n",
                "2.  **Semantic Clustering**: LSA helps find similarity even when documents don't share identical words.\n",
                "3.  **Speed**: LSA is linear algebra based and usually very fast to compute compared to probabilistic models like LDA.\n",
                "\n",
                "## Next steps:\n",
                "- Explore **Topic Modeling (LDA)** to see a probabilistic approach to finding themes.\n",
                "- Compare LSA results with word embeddings like Word2Vec."
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "nlp_course_env",
            "language": "python",
            "name": "nlp_course_env"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.11.14"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}