{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "anchor-section",
   "metadata": {},
   "source": [
    "# ðŸ“Œ Topic: Practical NLP - Building Complete Pipelines\n",
    "\n",
    "### What you will learn\n",
    "1. **Pipeline architecture** - How to combine multiple NLP techniques\n",
    "2. **Data flow** - From raw text to actionable insights\n",
    "3. **Common patterns** - Preprocessing, feature extraction, analysis\n",
    "4. **Technique integration** - When and how to use each technique\n",
    "5. **Performance optimization** - Speed vs. accuracy trade-offs\n",
    "6. **Production patterns** - Error handling, logging, monitoring\n",
    "\n",
    "### Why this matters\n",
    "Single techniques in isolation are useful for learning. Real systems combine multiple NLP tasks:\n",
    "- A document classifier uses tokenization + POS + NER + sentiment\n",
    "- A chatbot uses tokenization + POS + NER + sentiment + semantic similarity\n",
    "- A content recommender uses NER + sentiment + semantic search\n",
    "\n",
    "Understanding how techniques work together is essential for building robust production systems.\n",
    "\n",
    "### Real-world context\n",
    "When you search for \"angry reviews about Apple products\", the system:\n",
    "1. Tokenizes each review\n",
    "2. Extracts NER entities (Apple â†’ ORG)\n",
    "3. Analyzes sentiment (angry â†’ negative)\n",
    "4. Combines: (ORG=Apple) AND (sentiment=negative)\n",
    "5. Returns matching reviews\n",
    "\n",
    "This notebook shows how these techniques work together.\n",
    "\n",
    "---"
   ]
  },\n,
   "cell_type": "markdown",\n",
   "id": "practical-scenario",\n",
   "metadata": {},\n",
   "source": [
    "## Real-World Scenario: News Content Pipeline\n",
    "\n",
    "Build a **News Analysis System** that:\n",
    "1. **Extracts entities** (Who? Where? When?)\n",
    "2. **Detects sentiment** (Is the article positive/negative?)\n",
    "3. **Identifies key terms** (What words matter most?)\n",
    "4. **Analyzes grammar** (What type of text is this?)\n",
    "5. **Generates insights** (Automated summaries, categorization)\n",
    "\n",
    "This is what companies do when processing massive news feeds."
   ]
  },\n,
   "cell_type": "code",\n",
   "execution_count": null,\n",
   "id": "imports",\n",
   "metadata": {},\n",
   "outputs": [],\n",
    "source": [
    "# Complete NLP pipeline imports\n",
    "import spacy  # Modern NLP\n",
    "import nltk  # Classic NLP\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize  # Tokenization\n",
    "from nltk.corpus import stopwords  # Stop word removal\n",
    "from nltk.stem import WordNetLemmatizer  # Lemmatization\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer  # Sentiment\n",
    "\n",
    "import pandas as pd  # Data handling\n",
    "import matplotlib.pyplot as plt  # Visualization\n",
    "from collections import Counter  # Frequency\n",
    "import re  # Text cleaning\n",
    "\n",
    "# Initialize components\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "print(\"âœ“ All NLP components loaded\")"
   ]
  },\n,
   "cell_type": "markdown",\n",
   "id": "nlp-pipeline",\n",
   "metadata": {},\n",
   "source": [
    "## Step 1: Build the Complete NLP Pipeline\n",
    "\n",
    "Create a reusable pipeline that combines all techniques."
   ]
  },\n,
   "cell_type": "code",\n",
   "execution_count": null,\n",
   "id": "pipeline-class",\n",
   "metadata": {},\n",
   "outputs": [],\n",
    "source": [
    "class NLPPipeline:\n",
    "    \"\"\"\n",
    "    Complete NLP analysis pipeline combining:\n",
    "    - Tokenization\n",
    "    - POS tagging\n",
    "    - Named Entity Recognition\n",
    "    - Sentiment Analysis\n",
    "    - Lemmatization\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.nlp = spacy.load(\"en_core_web_sm\")\n",
    "        self.sia = SentimentIntensityAnalyzer()\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "    \n",
    "    def analyze(self, text):\n",
    "        \"\"\"\n",
    "        Complete analysis of text.\n",
    "        Returns: Dictionary with all analysis results\n",
    "        \"\"\"\n",
    "        # Process with spaCy\n",
    "        doc = self.nlp(text)\n",
    "        \n",
    "        # Extract entities\n",
    "        entities = [\n",
    "            {'text': ent.text, 'type': ent.label_}\n",
    "            for ent in doc.ents\n",
    "        ]\n",
    "        \n",
    "        # Tokenization and POS tagging\n",
    "        tokens = [\n",
    "            {'text': token.text, 'pos': token.pos_, 'lemma': token.lemma_}\n",
    "            for token in doc\n",
    "        ]\n",
    "        \n",
    "        # Sentiment analysis\n",
    "        sentiment = self.sia.polarity_scores(text)\n",
    "        \n",
    "        # Key terms (non-stop words, nouns/verbs)\n",
    "        key_terms = [\n",
    "            token.text.lower()\n",
    "            for token in doc\n",
    "            if token.text.lower() not in self.stop_words\n",
    "            and token.pos_ in ['NOUN', 'VERB', 'ADJ']\n",
    "        ]\n",
    "        \n",
    "        return {\n",
    "            'entities': entities,\n",
    "            'tokens': tokens,\n",
    "            'sentiment': sentiment,\n",
    "            'key_terms': key_terms,\n",
    "            'sentence_count': len(list(doc.sents)),\n",
    "            'token_count': len(doc)\n",
    "        }\n",
    "\n",
    "# Create pipeline instance\n",
    "pipeline = NLPPipeline()\n",
    "print(\"âœ“ Pipeline created and ready\")"
   ]
  },\n,
   "cell_type": "markdown",\n",
   "id": "test-pipeline",\n",
   "metadata": {},\n",
   "source": [
    "## Step 2: Test on Real Text\n",
    "\n",
    "Analyze a complete news article using the full pipeline."
   ]
  },\n,
   "cell_type": "code",\n",
   "execution_count": null,\n",
   "id": "test-article",\n",
   "metadata": {},\n",
   "outputs": [],\n",
    "source": [
    "# Sample article\n",
    "article = \"\"\"\n",
    "Apple Inc announced on January 15, 2024, that it has partnered with OpenAI to integrate \n",
    "advanced AI capabilities into its products. CEO Tim Cook praised the collaboration, calling it \n",
    "\"groundbreaking and transformative for our industry.\" The partnership will bring cutting-edge \n",
    "artificial intelligence to millions of Apple users worldwide. Market analysts were extremely positive \n",
    "about the announcement, with stock prices surging immediately following the news.\n",
    "\"\"\"\n",
    "\n",
    "# Analyze\n",
    "print(\"Analyzing article...\\n\")\n",
    "results = pipeline.analyze(article)\n",
    "\n",
    "print(f\"ARTICLE ANALYSIS REPORT\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Statistics\n",
    "print(f\"\\nBASIC STATISTICS:\")\n",
    "print(f\"  Sentences: {results['sentence_count']}\")\n",
    "print(f\"  Tokens: {results['token_count']}\")\n",
    "\n",
    "# Entities\n",
    "print(f\"\\nNAMED ENTITIES ({len(results['entities'])})\")\n",
    "for entity in results['entities']:\n",
    "    print(f\"  {entity['text']:25} â†’ {entity['type']}\")\n",
    "\n",
    "# Sentiment\n",
    "print(f\"\\nSENTIMENT ANALYSIS:\")\n",
    "sentiment = results['sentiment']\n",
    "print(f\"  Compound Score: {sentiment['compound']:>7.3f}\")\n",
    "print(f\"  Positive:       {sentiment['pos']:>7.1%}\")\n",
    "print(f\"  Neutral:        {sentiment['neu']:>7.1%}\")\n",
    "print(f\"  Negative:       {sentiment['neg']:>7.1%}\")\n",
    "label = 'POSITIVE' if sentiment['compound'] > 0.1 else 'NEGATIVE' if sentiment['compound'] < -0.1 else 'NEUTRAL'\n",
    "print(f\"  Label: {label}\")\n",
    "\n",
    "# Key terms\n",
    "print(f\"\\nTOP KEY TERMS:\")\n",
    "term_freq = Counter(results['key_terms'])\n",
    "for term, count in term_freq.most_common(10):\n",
    "    print(f\"  {term:20} â†’ {count} times\")"
   ]
  },\n,
   "cell_type": "markdown",\n",
   "id": "batch-processing",\n",
   "metadata": {},\n",
   "source": [
    "## Step 3: Batch Processing\n",
    "\n",
    "Scale to multiple articles efficiently."
   ]
  },\n,
   "cell_type": "code",\n",
   "execution_count": null,\n",
   "id": "batch-articles",\n",
   "metadata": {},\n",
   "outputs": [],\n",
    "source": [
    "# Multiple articles for batch processing\n",
    "articles = [\n",
    "    \"Apple and Microsoft announced a groundbreaking partnership. Investors are thrilled!\",\n",
    "    \"Tesla faces massive recalls amid safety concerns. Customers express frustration.\",\n",
    "    \"Amazon reported record earnings in Q4 2024. Stock price surged following announcement.\",\n",
    "    \"Meta's AI research division made significant advances in language models.\"\n",
    "]\n",
    "\n",
    "# Process all articles\n",
    "results_list = []\n",
    "\n",
    "print(\"Processing batch of articles...\\n\")\n",
    "for i, article in enumerate(articles, 1):\n",
    "    result = pipeline.analyze(article)\n",
    "    \n",
    "    results_list.append({\n",
    "        'article_id': i,\n",
    "        'text': article[:60] + '...',\n",
    "        'sentiment_score': result['sentiment']['compound'],\n",
    "        'entity_count': len(result['entities']),\n",
    "        'token_count': result['token_count'],\n",
    "        'top_entity': result['entities'][0]['text'] if result['entities'] else 'None'\n",
    "    })\n",
    "\n",
    "# Create DataFrame for analysis\n",
    "batch_results = pd.DataFrame(results_list)\n",
    "print(\"Batch Processing Results:\")\n",
    "print(batch_results.to_string(index=False))\n",
    "\n",
    "# Summary\n",
    "print(f\"\\nBATCH SUMMARY:\")\n",
    "print(f\"  Articles processed: {len(articles)}\")\n",
    "print(f\"  Average sentiment: {batch_results['sentiment_score'].mean():.3f}\")\n",
    "print(f\"  Total entities found: {batch_results['entity_count'].sum()}\")"
   ]
  },\n,
   "cell_type": "markdown",\n",
   "id": "visualization",\n",
   "metadata": {},\n",
   "source": [
    "## Step 4: Visualize Results\n",
    "\n",
    "Create charts for stakeholders."
   ]
  },\n,
   "cell_type": "code",\n",
   "execution_count": null,\n",
   "id": "viz-code",\n",
   "metadata": {},\n",
   "outputs": [],\n",
    "source": [
    "# Visualize batch analysis\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# Sentiment scores\n",
    "axes[0].bar(batch_results['article_id'], batch_results['sentiment_score'], \n",
    "             color=['green' if x > 0 else 'red' for x in batch_results['sentiment_score']])\n",
    "axes[0].set_title('Article Sentiment Scores', fontweight='bold')\n",
    "axes[0].set_xlabel('Article ID')\n",
    "axes[0].set_ylabel('Sentiment Score')\n",
    "axes[0].axhline(y=0, color='black', linestyle='--', alpha=0.3)\n",
    "\n",
    "# Entity count\n",
    "axes[1].bar(batch_results['article_id'], batch_results['entity_count'], color='steelblue')\n",
    "axes[1].set_title('Entities per Article', fontweight='bold')\n",
    "axes[1].set_xlabel('Article ID')\n",
    "axes[1].set_ylabel('Entity Count')\n",
    "\n",
    "# Token count\n",
    "axes[2].bar(batch_results['article_id'], batch_results['token_count'], color='coral')\n",
    "axes[2].set_title('Tokens per Article', fontweight='bold')\n",
    "axes[2].set_xlabel('Article ID')\n",
    "axes[2].set_ylabel('Token Count')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"âœ“ Visualizations complete\")"
   ]
  },\n,
   "cell_type": "markdown",\n",
   "id": "key-takeaways",\n",
   "metadata": {},\n",
   "source": [
    "## Key Takeaways\n",
    "\n",
    "1. **Pipelines orchestrate components**: Don't use techniques in isolation\n",
    "2. **Order matters**: Tokenize â†’ POS tag â†’ Lemmatize â†’ Analyze\n",
    "3. **Modular design scales**: Reusable classes handle different texts\n",
    "4. **Batch processing is essential**: Single-article analysis doesn't scale\n",
    "5. **Combine signals**: Entity + sentiment + key terms = rich insights\n",
    "6. **Visualization matters**: Charts communicate findings to non-technical stakeholders\n",
    "7. **Error handling**: Real-world text has edge casesâ€”handle them gracefully\n",
    "\n",
    "## What You've Learned (Full Course)\n",
    "\n",
    "### Fundamentals (Notebooks 1-6)\n",
    "- âœ… Tokenization\n",
    "- âœ… Lowercasing & normalization\n",
    "- âœ… Stemming vs. lemmatization\n",
    "- âœ… N-grams\n",
    "- âœ… Parts of speech\n",
    "\n",
    "### Advanced (Notebooks 7-11)\n",
    "- âœ… Named Entity Recognition\n",
    "- âœ… Entity extraction at scale\n",
    "- âœ… Sentiment analysis\n",
    "- âœ… Practical applications\n",
    "- âœ… Complete pipelines\n",
    "\n",
    "## Next Steps (After This Course)\n",
    "1. **Transformers**: BERT, GPT, RoBERTa for state-of-the-art results\n",
    "2. **Custom models**: Train domain-specific models\n",
    "3. **Production deployment**: REST APIs, Docker containers, cloud infrastructure\n",
    "4. **Scale**: Handle billions of documents\n",
    "5. **Advanced NLP**: Machine translation, question answering, summarization\n",
    "\n",
    "---\n",
    "\n",
    "## Congratulations! ðŸŽ‰\n",
    "\n",
    "You've learned the complete NLP fundamentals pipeline. You can now:\n",
    "- Process raw text into structured insights\n",
    "- Build production NLP systems\n",
    "- Understand trade-offs and limitations\n",
    "- Apply NLP to real-world problems\n",
    "\n",
    "The field evolves rapidly, but these fundamentals never change. Build on them!"
   ]
  }
 ],\n "metadata": {
  "kernelspec": {
   "display_name": "nlp_course_env",
   "language": "python",
   "name": "nlp_course_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
