{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "anchor-section",
   "metadata": {},
   "source": [
    "# üìå Topic: Part-of-Speech (POS) Tagging\n",
    "\n",
    "### What you will learn\n",
    "- What POS tags are and why they matter\n",
    "- How to use spaCy for automatic POS tagging\n",
    "- Understanding different POS tag categories\n",
    "- Analyzing word distributions by POS\n",
    "- Practical applications of POS tagging\n",
    "\n",
    "### Why this matters\n",
    "POS tagging automatically identifies the grammatical role of each word (noun, verb, adjective, etc.). This is essential for many NLP tasks: dependency parsing, named entity recognition, sentiment analysis, and grammar checking. spaCy makes this easy with pre-trained models that achieve 97%+ accuracy on English text.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": [
    "## What are POS Tags?\n",
    "\n",
    "**Part-of-speech (POS) tags** label each word with its grammatical role in context.\n",
    "\n",
    "### Example:\n",
    "```\n",
    "\"The cat sleeps peacefully\"\n",
    " |   |    |      |  \n",
    " DET NOUN VERB   ADV\n",
    "```\n",
    "\n",
    "### Common POS Tags:\n",
    "\n",
    "| Tag | Meaning | Examples |\n",
    "|-----|---------|----------|\n",
    "| **NOUN** | Noun | cat, dog, house, intelligence |\n",
    "| **VERB** | Verb | sleep, eat, run, think |\n",
    "| **ADJ** | Adjective | happy, red, intelligent |\n",
    "| **ADV** | Adverb | quickly, very, peacefully |\n",
    "| **PROPN** | Proper Noun | Emma, London, Google |\n",
    "| **PRON** | Pronoun | I, he, she, it, they |\n",
    "| **DET** | Determiner | the, a, an, this, that |\n",
    "| **ADP** | Adposition (preposition) | in, on, at, from |\n",
    "| **AUX** | Auxiliary Verb | is, was, have, will |\n",
    "| **CCONJ** | Coordinating Conjunction | and, or, but |\n",
    "| **SCONJ** | Subordinating Conjunction | because, if, while |\n",
    "| **INTJ** | Interjection | wow, oops, hello |\n",
    "| **NUM** | Number | 1, two, 3.14 |\n",
    "| **PART** | Particle | 's, not, -ing (suffix) |\n",
    "\n",
    "### Why POS tags matter:\n",
    "1. **Disambiguate meaning**: \"bank\" (NOUN) vs. \"bank\" (VERB) vs. \"bank\" (ADP in \"bank on\")\n",
    "2. **Enable syntax analysis**: Build parse trees, identify sentence structure\n",
    "3. **Improve downstream tasks**: NER, sentiment analysis, lemmatization all benefit from POS\n",
    "4. **Grammar checking**: Detect grammatical errors based on POS sequences\n",
    "5. **Information extraction**: Find nouns for entity names, verbs for actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,\n",
   "id": "efc354fd-acd3-48e0-b2ce-f5782223ff0b",\n",
   "metadata": {},\n",
   "outputs": [],\n",
   "source": [
    "# Import spaCy - modern NLP library with pre-trained models\n",
    "# spaCy includes POS tagging, NER, dependency parsing, and more\n",
    "import spacy\n",
    "\n",
    "# Import pandas for data manipulation and analysis\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spacy-intro",\n",
   "metadata": {},\n",
   "source": [
    "## Introduction to spaCy\n",
    "\n",
    "**spaCy** is a production-grade NLP library optimized for speed and accuracy. Unlike NLTK (rule-based), spaCy uses neural networks trained on real data.\n",
    "\n",
    "### Key advantages:\n",
    "- **Speed**: Fast neural models (process millions of words per second)\n",
    "- **Accuracy**: 97%+ accuracy on English POS tagging\n",
    "- **Integrated pipeline**: Tokenization ‚Üí POS ‚Üí NER ‚Üí Dependency parsing all in one\n",
    "- **Production-ready**: Used by companies at scale\n",
    "- **Multiple languages**: English, German, French, etc.\n",
    "\n",
    "### Pre-trained models:\n",
    "- `en_core_web_sm`: Small, fast, ~13 MB (good for learning)\n",
    "- `en_core_web_md`: Medium, ~42 MB (word vectors included)\n",
    "- `en_core_web_lg`: Large, ~746 MB (best accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,\n",
   "id": "aa9bcb08-21c0-4ba7-9277-ba6cb7151a91",\n",
   "metadata": {},\n",
   "outputs": [],\n",
   "source": [
    "# Load the pre-trained English model\n",
    "# This model is trained on real English text and understands grammar/structure\n",
    "# Download first with: python -m spacy download en_core_web_sm\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "print(\"‚úì spaCy model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "text-processing",\n",
   "metadata": {},\n",
   "source": [
    "## Step 1: Process Text with spaCy\n",
    "\n",
    "When you pass text through spaCy, it creates a **Doc object** containing:\n",
    "- Tokens (words)\n",
    "- POS tags\n",
    "- Lemmas\n",
    "- Named entities\n",
    "- Dependency information\n",
    "\nAll this happens automatically!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,\n",
   "id": "422befbd-2f7b-45f3-85d7-2f016b6caa71",\n",
   "metadata": {},\n",
   "outputs": [],\n",
   "source": [
    "# Sample text from Jane Austen's \"Emma\" (opening paragraph)\n",
    "# Pre-processed to lowercase (you'd normally do this in preprocessing step)\n",
    "emma_text = (\n",
    "    \"emma woodhouse handsome clever and rich with a comfortable home and happy disposition \"\n",
    "    \"seemed to unite some of the best blessings of existence and had lived nearly twenty-one \"\n",
    "    \"years in the world with very little to distress or vex her she was the youngest of the two \"\n",
    "    \"daughters of a most affectionate indulgent father and had in consequence of her sisters marriage \"\n",
    "    \"been mistress of his house from a very early period her mother had died too long ago for her to \"\n",
    "    \"have more than an indistinct remembrance of her caresses and her place had been supplied by an \"\n",
    "    \"excellent woman as governess who had fallen little short of a mother in affection sixteen years \"\n",
    "    \"had miss taylor been in mr woodhouses family less as a governess than a friend very fond of both \"\n",
    "    \"daughters but particularly of emma between them it was more the intimacy of sisters even before \"\n",
    "    \"miss taylor had ceased to hold the nominal office of governess\"\n",
    ")\n",
    "\n",
    "print(f\"Sample text (first 100 chars): {emma_text[:100]}...\")\n",
    "print(f\"Total length: {len(emma_text)} characters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,\n",
   "id": "cf5d8942-6770-4104-a5cc-07ef37d5d2e1",\n",
   "metadata": {},\n",
   "outputs": [],\n",
   "source": [
    "# Pass text through spaCy pipeline\n",
    "# This triggers tokenization, POS tagging, lemmatization, NER, dependency parsing\n",
    "# The nlp() call returns a Doc object containing all linguistic information\n",
    "spacy_doc = nlp(emma_text)\n",
    "\n",
    "print(f\"‚úì Processed {len(spacy_doc)} tokens\")\n",
    "print(f\"\\nFirst 10 tokens and their POS tags:\")\n",
    "for i, token in enumerate(spacy_doc[:10]):\n",
    "    print(f\"  {token.text:15} ‚Üí {token.pos_:10} (lemma: {token.lemma_})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "creating-dataframe",\n",
   "metadata": {},\n",
   "source": [
    "## Step 2: Extract POS Tags into DataFrame\n",
    "\n",
    "Let's create a structured table (DataFrame) with tokens and their POS tags for easy analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,\n",
   "id": "4f45e487-980e-4dd3-a78c-8a4db821ec36",\n",
   "metadata": {},\n",
   "outputs": [],\n",
   "source": [
    "# Create a DataFrame from the spaCy Doc\n",
    "# Extract token text and POS tag for each word\n",
    "# Use list comprehension to iterate through all tokens\n",
    "pos_df = pd.DataFrame([\n",
    "    {\"token\": token.text, \"pos_tag\": token.pos_}\n",
    "    for token in spacy_doc\n",
    "])\n",
    "\n",
    "# Display first 10 and last 10 rows\n",
    "print(\"First 10 tokens:\")\n",
    "print(pos_df.head(10))\n",
    "print(\"\\n...\\n\")\n",
    "print(\"Last 10 tokens:\")\n",
    "print(pos_df.tail(10))\n",
    "print(f\"\\nTotal tokens: {len(pos_df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "frequency-analysis",\n",
   "metadata": {},\n",
   "source": [
    "## Step 3: Frequency Analysis\n",
    "\n",
    "Now let's analyze which words appear most frequently for each POS category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,\n",
   "id": "dbf20645-b4b6-433f-9d08-f9452bce1fb0",\n",
   "metadata": {},\n",
   "outputs": [],\n",
   "source": [
    "# Group by token and POS tag, count occurrences\n",
    "# This shows: which specific words appear, how many times, and their POS\n",
    "pos_token_counts = (\n",
    "    pos_df.groupby([\"token\", \"pos_tag\"])\n",
    "    .size()\n",
    "    .reset_index(name=\"count\")\n",
    "    .sort_values(by=\"count\", ascending=False)\n",
    ")\n",
    "\n",
    "print(\"Top 15 Most Frequent Token-POS Combinations:\")\n",
    "print(\"=\" * 50)\n",
    "print(pos_token_counts.head(15))\n",
    "\n",
    "print(\"\\nInsight: Function words (of, had, her, the, and) dominate\")\n",
    "print(\"These are stop words carrying little semantic meaning\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pos-distribution",\n",
   "metadata": {},\n",
   "source": [
    "## Step 4: POS Distribution\n",
    "\n",
    "Let's see how many words we have in each POS category (ignore specific words, just count categories)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,\n",
   "id": "c68a6201-37e3-4bba-8f8e-1c8ea57110dc",\n",
   "metadata": {},\n",
   "outputs": [],\n",
   "source": [
    "# Count unique words in each POS category\n",
    "# This shows: how many different nouns, verbs, adjectives, etc. appear\n",
    "pos_distribution = (\n",
    "    pos_token_counts.groupby(\"pos_tag\")[\"token\"]\n",
    "    .count()\n",
    "    .sort_values(ascending=False)\n",
    ")\n",
    "\n",
    "print(\"POS Tag Distribution (number of unique words per category):\")\n",
    "print(\"=\" * 55)\n",
    "for pos, count in pos_distribution.items():\n",
    "    print(f\"{pos:12} {count:3} unique words\")\n",
    "\n",
    "print(f\"\\n‚úì Total unique POS categories: {len(pos_distribution)}\")\n",
    "print(f\"‚úì Most common: {pos_distribution.idxmax()} ({pos_distribution.max()} words)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "noun-analysis",\n",
   "metadata": {},\n",
   "source": [
    "## Step 5: Deep-Dive into Nouns\n",
    "\n",
    "Let's examine the most frequent nouns, which are semantically important."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,\n",
   "id": "b420e84e-0b81-4314-929e-ed40141b79aa",\n",
   "metadata": {},\n",
   "outputs": [],\n",
   "source": [
    "# Filter for nouns only and show top 10\n",
    "# Nouns are semantically rich (unlike function words)\n",
    "top_nouns = pos_token_counts[pos_token_counts.pos_tag == \"NOUN\"].head(10)\n",
    "\n",
    "print(\"Top 10 Most Frequent Nouns:\")\n",
    "print(\"=\" * 50)\n",
    "print(top_nouns[[\"token\", \"count\"]].to_string(index=False))\n",
    "\n",
    "print(\"\\nInsight: 'governess', 'friend', 'mother' are key concepts in the text\")\n",
    "print(\"These nouns relate to female relationships (major theme of Emma)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "advanced-analysis",\n",
   "metadata": {},\n",
   "source": [
    "## Step 6: Analyzing All POS Categories\n",
    "\n",
    "Let's get a complete view of what POS tags appear and their frequencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,\n",
   "id": "advanced-pos",\n",
   "metadata": {},\n",
   "outputs": [],\n",
   "source": [
    "# Show top 5 examples from each POS category\n",
    "print(\"Examples from Each POS Category:\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for pos in pos_distribution.index[:8]:  # Show top 8 categories\n",
    "    examples = pos_token_counts[pos_token_counts.pos_tag == pos].head(3)\n",
    "    words = \", \".join([f\"{row['token']} ({row['count']})\" \n",
    "                       for _, row in examples.iterrows()])\n",
    "    print(f\"\\n{pos:10} ({pos_distribution[pos]:2} words) ‚Üí {words}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "practical-uses",\n",
   "metadata": {},\n",
   "source": [
    "## Practical Applications of POS Tagging\n",
    "\n",
    "### 1. **Named Entity Recognition (NER)**\n",
    "Proper nouns (PROPN) often indicate people, places, organizations.\n",
    "```python\n",
    "if token.pos_ == \"PROPN\":\n",
    "    # Likely an entity, do further analysis\n",
    "```\n",
    "\n",
    "### 2. **Information Extraction**\n",
    "Extract subject-verb-object patterns:\n",
    "```\n",
    "NOUN(subject) + VERB + NOUN(object)\n",
    "Example: \"Emma loved books\" ‚Üí Extract fact about Emma\n",
    "```\n",
    "\n",
    "### 3. **Sentiment Analysis**\n",
    "Focus on adjectives and adverbs:\n",
    "```python\n",
    "if token.pos_ in [\"ADJ\", \"ADV\"]:\n",
    "    # Check if word is positive/negative\n",
    "```\n",
    "\n",
    "### 4. **Grammar Checking**\n",
    "Detect grammatical errors:\n",
    "```\n",
    "NOUN + NOUN + VERB ‚Üí Usually incorrect (should be adj between)\n",
    "\"blue house is\" ‚Üí Check if \"blue\" is actually ADJ\n",
    "```\n",
    "\n",
    "### 5. **Lemmatization Improvement**\n",
    "POS tags help lemmatizers disambiguate:\n",
    "```python\n",
    "# \"better\" is ambiguous without context\n",
    "lemmatizer.lemmatize(\"better\", pos=\"ADJ\")  # ‚Üí \"good\"\n",
    "```\n",
    "\n",
    "### 6. **Machine Translation**\n",
    "Target language needs correct POS structure:\n",
    "```\n",
    "English: \"big red house\" (ADJ ADJ NOUN)\n",
    "French: \"maison grande rouge\" (NOUN ADJ ADJ) ‚Üê different order!\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "common-mistakes",\n",
   "metadata": {},\n",
   "source": [
    "## Common Beginner Mistakes\n",
    "\n",
    "### ‚ùå Mistake 1: Assuming word form determines POS\n",
    "```python\n",
    "\"The bank is running\" vs. \"The bank of the river\"\n",
    "# 'bank' looks like a noun but first example has VERB 'is running' after it\n",
    "# Wrong: Always treating 'bank' as NOUN\n",
    "# Right: Let the model determine POS from context\n",
    "```\n",
    "\n",
    "### ‚ùå Mistake 2: Using POS tags without understanding context\n",
    "```python\n",
    "# Just because something is marked VERB doesn't mean it's an action\n",
    "# \"seems\" is a verb but expresses a state, not an action\n",
    "# \"The data seems to show\" ‚Üê 'seems' is important for sentiment\n",
    "```\n",
    "\n",
    "### ‚ùå Mistake 3: Expecting 100% accuracy\n",
    "```python\n",
    "# Even spaCy gets ~97% accuracy\n",
    "# \"The bear walked to the bank\" ‚Üê may tag wrong without context\n",
    "# Design systems that tolerate occasional errors\n",
    "```\n",
    "\n",
    "### ‚úÖ Best practices:\n",
    "- Always use POS tags, not just word forms\n",
    "- Understand that POS is context-dependent\n",
    "- Design error-tolerant systems\n",
    "- Combine with other signals (lemma, entity type, etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "key-takeaways",\n",
   "metadata": {},\n",
   "source": [
    "## Key Takeaways\n",
    "\n",
    "1. **POS tags identify grammar roles**: NOUN, VERB, ADJ, ADV, etc.\n",
    "2. **Context matters**: Same word can have different POS (\"bank\" as NOUN vs. VERB)\n",
    "3. **spaCy makes it easy**: Pre-trained models with 97%+ accuracy\n",
    "4. **Function words dominate frequency**: Stop words appear most often\n",
    "5. **Content words matter more**: Nouns/verbs/adjectives carry meaning\n",
    "6. **POS enables downstream tasks**: NER, sentiment analysis, parsing, etc.\n",
    "7. **Pandas makes analysis easy**: Group, filter, and visualize POS data\n",
    "\n",
    "## Next Steps:\n",
    "- Learn **dependency parsing** (understanding word relationships)\n",
    "- Study **named entity recognition** (finding people, places, things)\n",
    "- Explore **sentence structure analysis** using POS sequences"
   ]
  },\n",
   "cell_type": "markdown",\n",
   "id": "practice",\n",
   "metadata": {},\n",
   "source": [
    "## Practice Exercise\n",
    "\n",
    "Try these on your own:"
   ]
  },\n",
   "cell_type": "code",\n",
   "execution_count": null,\n",
   "id": "practice-1",\n",
   "metadata": {},\n",
   "outputs": [],\n",
   "source": [
    "# Exercise 1: Find all adjectives and adverbs (sentiment-bearing words)\n",
    "sentiment_words = pos_token_counts[\n",
    "    pos_token_counts.pos_tag.isin([\"ADJ\", \"ADV\"])\n",
    "].head(10)\n",
    "\n",
    "print(\"Adjectives and Adverbs (potential sentiment words):\")\n",
    "for _, row in sentiment_words.iterrows():\n",
    "    print(f\"  {row['token']:15} ({row['pos_tag']:3}) - appears {row['count']} times\")"
   ]
  },\n",
   "cell_type": "code",\n",
   "execution_count": null,\n",
   "id": "practice-2",\n",
   "metadata": {},\n",
   "outputs": [],\n",
   "source": [
    "# Exercise 2: Analyze a new sentence with POS tags\n",
    "test_text = \"Emma loves reading novels about relationships\"\n",
    "test_doc = nlp(test_text)\n",
    "\n",
    "print(f\"Analyzing: '{test_text}'\")\n",
    "print(\"=\" * 50)\n",
    "for token in test_doc:\n",
    "    print(f\"{token.text:15} ‚Üí {token.pos_:8} (lemma: {token.lemma_})\")"
   ]
  },\n",
   "cell_type": "code",\n",
   "execution_count": null,\n",
   "id": "practice-3",\n",
   "metadata": {},\n",
   "outputs": [],\n",
   "source": [
    "# Exercise 3: Create a simple POS-based filter\n",
    "# Extract only content words (nouns, verbs, adjectives, adverbs)\n",
    "content_pos = [\"NOUN\", \"VERB\", \"ADJ\", \"ADV\", \"PROPN\"]\n",
    "content_words = pos_token_counts[\n",
    "    pos_token_counts.pos_tag.isin(content_pos)\n",
    "].sort_values(by=\"count\", ascending=False)\n",
    "\n",
    "print(f\"Content words (non-function words): {len(content_words)} unique\")\n",
    "print(\"\\nTop 10 content words:\")\n",
    "for _, row in content_words.head(10).iterrows():\n",
    "    print(f\"  {row['token']:15} ({row['pos_tag']:5}) - {row['count']} times\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_course_env",
   "language": "python",
   "name": "nlp_course_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
