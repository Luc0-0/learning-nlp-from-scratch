{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "anchor-section",
   "metadata": {},
   "source": [
    "# üìå Topic: Lemmatization\n",
    "\n",
    "### What you will learn\n",
    "- What lemmatization is and how it differs from stemming\n",
    "- How the WordNet Lemmatizer works\n",
    "- When to use lemmatization vs. stemming\n",
    "- Practical applications: accuracy over speed\n",
    "- How lemmatization preserves word meaning\n",
    "\n",
    "### Why this matters\n",
    "Lemmatization is the **smarter, slower cousin of stemming**. Instead of blindly stripping suffixes, it uses a dictionary (WordNet) to find the actual root word (lemma). While slower, it produces real English words and preserves semantic meaning, making it essential for accurate NLP tasks like sentiment analysis, NER, and question answering.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": [
    "## What is Lemmatization?\n",
    "\n",
    "**Lemmatization** is the process of reducing words to their base form (called the **lemma**) using morphological analysis and a dictionary.\n",
    "\n",
    "### Example:\n",
    "- \"connecting\", \"connected\", \"connects\" ‚Üí \"connect\"\n",
    "- \"better\" ‚Üí \"good\" (not \"better\"!)\n",
    "- \"was\", \"is\", \"are\" ‚Üí \"be\"\n",
    "\n",
    "### Key differences from stemming:\n",
    "\n",
    "| Feature | Stemming | Lemmatization |\n",
    "|---------|----------|----------------|\n",
    "| **Approach** | Rule-based suffix stripping | Dictionary lookup |\n",
    "| **Speed** | ‚ö° Fast | üê¢ Slow |\n",
    "| **Output** | May be non-words | Always real words |\n",
    "| **Accuracy** | Lower | ‚úÖ Higher |\n",
    "| **\"better\"** | \"better\" | \"good\" |\n",
    "| **\"was\"** | \"wa\" (non-word) | \"be\" |\n",
    "\n",
    "### Why choose lemmatization?\n",
    "1. **Semantically correct**: \"better\" actually means \"good in comparative form\"\n",
    "2. **Real words only**: No garbage like \"poni\" or \"wa\"\n",
    "3. **Better for downstream tasks**: NER, sentiment analysis benefit from accurate lemmas\n",
    "4. **Improves interpretability**: You're working with actual English words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14adda7c-955e-4fc0-a39f-94a968362bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatization reduces a word to its base form (lemma) while preserving semantic meaning\n",
    "# Unlike stemming, it uses a dictionary to find the actual root word\n",
    "# Example: \"better\" ‚Üí \"good\" (not \"better\")\n",
    "# Example: \"was\" ‚Üí \"be\" (not \"wa\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff57e906-688c-4ed7-961e-8cf1128f23b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import NLTK and download the WordNet corpus\n",
    "# WordNet is a large lexical database of English words with relationships\n",
    "# It contains lemmas, definitions, and relationships between words\n",
    "import nltk\n",
    "nltk.download('wordnet')  # Download once per environment\n",
    "nltk.download('averaged_perceptron_tagger')  # For part-of-speech tagging (optional but helpful)\n",
    "\n",
    "# Import the WordNetLemmatizer\n",
    "# This lemmatizer uses WordNet to find the correct base form of words\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "instantiation",
   "metadata": {},
   "source": [
    "## Creating a Lemmatizer Instance\n",
    "\n",
    "To use lemmatization, we create an instance of the `WordNetLemmatizer` class. This object has a `.lemmatize()` method that converts words to their lemmas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d10746a-2622-4fd6-bef2-fde0d24bdd9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of the WordNetLemmatizer\n",
    "# This object will use WordNet to look up lemmas\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "basic-example",
   "metadata": {},
   "source": [
    "## Basic Lemmatization Example\n",
    "\n",
    "Let's lemmatize the same word family we stemmed before (connect, connecting, connected, etc.) and compare results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "connect-lemmatization",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of connect-related words to lemmatize\n",
    "connect_words = [\"connecting\", \"connected\", \"connectivity\", \"connects\"]\n",
    "\n",
    "# Lemmatize each word\n",
    "# Note: .lemmatize() is simpler than stemming - just pass the word\n",
    "print(\"WordNet Lemmatizer Output:\")\n",
    "print(\"=\" * 45)\n",
    "for word in connect_words:\n",
    "    # .lemmatize() returns the base form from WordNet dictionary\n",
    "    lemma = lemmatizer.lemmatize(word)\n",
    "    print(f\"{word:15} ‚Üí {lemma}\")\n",
    "\n",
    "# Notice: All reduce to \"connect\" (same as stemmer in this case)\n",
    "# But lemmatizer always returns real English words!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "advanced-examples",
   "metadata": {},
   "source": [
    "## Advanced Examples: Where Lemmatization Shines\n",
    "\n",
    "Lemmatization is smarter for tricky cases where stemming fails or produces non-words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "advanced-example-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate where lemmatization outperforms stemming\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Test words where stemming and lemmatization differ significantly\n",
    "test_words = [\"better\", \"ponies\", \"was\", \"arguing\", \"universal\"]\n",
    "\n",
    "print(\"Stemming vs. Lemmatization Comparison:\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"{'Word':<15} | {'Stemmed':<15} | {'Lemmatized':<15} | {'Status':<10}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for word in test_words:\n",
    "    stem = stemmer.stem(word)\n",
    "    lemma = lemmatizer.lemmatize(word)\n",
    "    \n",
    "    # Check if results differ\n",
    "    status = \"‚úì Same\" if stem == lemma else \"‚úì Different\"\n",
    "    print(f\"{word:<15} | {stem:<15} | {lemma:<15} | {status:<10}\")\n",
    "\n",
    "print(\"\\n‚úì Key insight: 'better' ‚Üí 'good' (lemmatization) is semantically correct!\")\n",
    "print(\"‚úì Key insight: 'ponies' ‚Üí 'pony' (lemmatization) is a real word!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pos-tagging",
   "metadata": {},
   "source": [
    "## Advanced: Part-of-Speech (POS) Tagging with Lemmatization\n",
    "\n",
    "Lemmatizers work even better when you tell them the **part of speech** (noun, verb, adjective, etc.). This helps disambiguate words with multiple lemmas.\n",
    "\n",
    "### Example:\n",
    "- \"leads\" (verb) ‚Üí \"lead\"\n",
    "- \"leads\" (noun) ‚Üí \"lead\" (same in this case)\n",
    "- But for \"reading\" ‚Üí \"read\" (verb) or \"reading\" (noun) is different context\n",
    "\n",
    "### POS Tags:\n",
    "- `v` = verb\n",
    "- `n` = noun\n",
    "- `a` = adjective\n",
    "- `r` = adverb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pos-tagging-example",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatization with part-of-speech hints\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Same word can have different lemmas depending on part of speech\n",
    "# Syntax: lemmatizer.lemmatize(word, pos='v')  # 'v' for verb, 'n' for noun, 'a' for adjective\n",
    "\n",
    "print(\"Lemmatization with POS Tags:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Example 1: \"running\"\n",
    "print(\"\\n'running' (as verb):\")\n",
    "print(f\"  Without POS: {lemmatizer.lemmatize('running')}\")\n",
    "print(f\"  With POS (v): {lemmatizer.lemmatize('running', pos='v')}\")\n",
    "\n",
    "# Example 2: \"better\"\n",
    "print(\"\\n'better' (as adjective):\")\n",
    "print(f\"  Without POS: {lemmatizer.lemmatize('better')}\")\n",
    "print(f\"  With POS (a): {lemmatizer.lemmatize('better', pos='a')}\")\n",
    "\n",
    "# Example 3: \"studies\"\n",
    "print(\"\\n'studies' (as verb vs noun):\")\n",
    "print(f\"  As verb (v): {lemmatizer.lemmatize('studies', pos='v')}\")\n",
    "print(f\"  As noun (n): {lemmatizer.lemmatize('studies', pos='n')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "complete-workflow",
   "metadata": {},
   "source": [
    "## Complete Workflow: Tokenization ‚Üí Lemmatization\n",
    "\n",
    "In practice, you'll combine tokenization with lemmatization. Here's the full pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "complete-pipeline",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Sample text\n",
    "text = \"The better artists were arguing about the universal laws\"\n",
    "\n",
    "# Step 1: Tokenize into words\n",
    "tokens = word_tokenize(text.lower())  # Lowercase first\n",
    "\n",
    "# Step 2: Lemmatize each token\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "\n",
    "# Display the pipeline\n",
    "print(\"Complete Preprocessing Pipeline:\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nOriginal text:\")\n",
    "print(f\"  {text}\")\n",
    "\n",
    "print(f\"\\nTokenized (lowercase):\")\n",
    "print(f\"  {tokens}\")\n",
    "\n",
    "print(f\"\\nAfter lemmatization:\")\n",
    "print(f\"  {lemmatized}\")\n",
    "\n",
    "print(f\"\\nKey transformations:\")\n",
    "print(f\"  'better' ‚Üí '{lemmatizer.lemmatize('better', pos='a')}'\")\n",
    "print(f\"  'artists' ‚Üí '{lemmatizer.lemmatize('artists', pos='n')}'\")\n",
    "print(f\"  'arguing' ‚Üí '{lemmatizer.lemmatize('arguing', pos='v')}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "when-to-use",
   "metadata": {},
   "source": [
    "## When to Use Lemmatization\n",
    "\n",
    "### ‚úÖ Good use cases:\n",
    "1. **Sentiment Analysis**: \"better\" ‚Üí \"good\" preserves semantic meaning\n",
    "2. **Named Entity Recognition**: Helps identify base forms in context\n",
    "3. **Information Extraction**: Accurate base forms for database storage\n",
    "4. **Small to medium datasets**: When accuracy matters more than speed\n",
    "5. **Text analysis tools**: Tools where interpretability is important\n",
    "\n",
    "### ‚ùå When NOT to use:\n",
    "1. **Billions of documents**: Too slow (lemmatization is 10-100x slower than stemming)\n",
    "2. **Real-time applications**: API responses need < 100ms latency\n",
    "3. **Modern neural networks**: Transformers learn morphology themselves (no need to lemmatize)\n",
    "4. **Character-level models**: They work at sub-word level\n",
    "\n",
    "### Rule of thumb:\n",
    "**Use lemmatization for accuracy-critical NLP tasks.** Use stemming for speed-critical applications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "limitations",
   "metadata": {},
   "source": [
    "## Limitations of WordNet Lemmatization\n",
    "\n",
    "Even lemmatization has limitations:\n",
    "\n",
    "### ‚ö†Ô∏è Problem 1: Default POS is noun\n",
    "Without specifying POS, WordNet assumes the word is a noun, which can be wrong.\n",
    "\n",
    "```python\n",
    "lemmatizer.lemmatize('running')  # Returns 'running' (assumes noun)\n",
    "lemmatizer.lemmatize('running', pos='v')  # Returns 'run' (correct!)\n",
    "```\n",
    "\n",
    "### ‚ö†Ô∏è Problem 2: WordNet is English-focused\n",
    "Works great for English, but limited support for other languages.\n",
    "\n",
    "### ‚ö†Ô∏è Problem 3: Domain-specific words not in WordNet\n",
    "Technical terms, slang, or neologisms won't lemmatize correctly.\n",
    "\n",
    "### ‚úÖ Solutions:\n",
    "- Use spaCy's lemmatizer (automatic POS tagging)\n",
    "- Build domain-specific lemmatization dictionaries\n",
    "- For non-English, use language-specific tools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spacy-preview",
   "metadata": {},
   "source": [
    "## Preview: spaCy's Superior Lemmatization\n",
    "\n",
    "spaCy is a modern NLP library that **automatically tags part-of-speech** before lemmatization. This makes it smarter than WordNet Lemmatizer.\n",
    "\n",
    "```python\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "doc = nlp(\"The better artists were arguing\")\n",
    "for token in doc:\n",
    "    print(f\"{token.text} ‚Üí {token.lemma_}\")\n",
    "# Output:\n",
    "# The ‚Üí the\n",
    "# better ‚Üí good (automatically detects adjective!)\n",
    "# artists ‚Üí artist\n",
    "# were ‚Üí be\n",
    "# arguing ‚Üí argue\n",
    "```\n",
    "\n",
    "We'll cover spaCy in a future notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "key-takeaways",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "1. **Lemmatization is smarter than stemming**: Uses dictionaries, not just rules\n",
    "2. **Always produces real words**: No garbage output like stemming\n",
    "3. **Semantically correct**: \"better\" ‚Üí \"good\", not \"better\"\n",
    "4. **Slower but more accurate**: Trade-off is speed vs. quality\n",
    "5. **POS tags improve accuracy**: Tell the lemmatizer if word is verb/noun/adjective\n",
    "6. **WordNet Lemmatizer has limitations**: Default POS is noun, only works well in English\n",
    "7. **spaCy is better for production**: Automatic POS tagging + better accuracy\n",
    "\n",
    "## Next Steps:\n",
    "- Learn about **parts of speech (POS) tagging**\n",
    "- Explore **spaCy for production-quality lemmatization**\n",
    "- Compare lemmatization across different tasks (sentiment analysis vs. text classification)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comparison-table",
   "metadata": {},
   "source": [
    "## Summary: Stemming vs. Lemmatization\n",
    "\n",
    "| Aspect | Stemming | Lemmatization |\n",
    "|--------|----------|----------------|\n",
    "| **Algorithm** | Rule-based suffix stripping | Dictionary lookup + morphology |\n",
    "| **Speed** | ‚ö°‚ö°‚ö° Very fast | üê¢ Slower |\n",
    "| **Output quality** | May be non-words | Always real words |\n",
    "| **Semantic accuracy** | Lower | ‚úÖ Higher |\n",
    "| **\"better\"** | \"better\" | \"good\" |\n",
    "| **\"ponies\"** | \"poni\" | \"pony\" |\n",
    "| **Best for** | Large-scale search, IR | Sentiment analysis, NER, accuracy-critical |\n",
    "| **Implementation** | Porter, Snowball | WordNet, spaCy |\n",
    "\n",
    "**Remember**: Choose based on your task's requirements for speed vs. accuracy."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_course_env",
   "language": "python",
   "name": "nlp_course_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
